{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9ni4307j+cT5DJOdB6e0v"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Illustrating DeepSeek-R1-Zero's Reinforcement Learning Innovations\n",
        "\n",
        "copyright 2025, Denis Rothman\n",
        "\n",
        "*Reference*:\n",
        "[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
        "Reinforcement Learning, January 2025](https://arxiv.org/pdf/2501.12948)"
      ],
      "metadata": {
        "id": "k4CV4u2VtOIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a simplified, educational example to help understand the core concepts behind DeepSeek-R1-Zero's approach of using Reinforcement Learning (RL) directly on a base model, as described in Section 2.2 of the [DeepSeek-R1 paper](https://arxiv.org/pdf/2501.12948).\n",
        "\n",
        " We will use the calculation of Gini impurity, a common concept in decision trees and machine learning, as our illustrative task.Disclaimer: This example uses a highly simplified \"model\" and \"task\" for illustrative purposes. It does not represent the complexity or scale of actual LLM training. The goal is to demonstrate the ideas of rule-based rewards and iterative learning in an RL context using a more relevant technical example.\n",
        "\n",
        " **I.Background**: Pure RL traditional approaches to training powerful language models often involve a multi-stage process:\n",
        "\n",
        " **1.Pre-training**: Training on a massive text dataset to learn language fundamentals.Supervised\n",
        "\n",
        " **2.Fine-Tuning (SFT)**: Training on a smaller dataset of high-quality examples (e.g., question-answer pairs, instructions) to align the model's output with desired formats and behaviors.\n",
        "\n",
        " **3.Reinforcement Learning (RL)**: Further fine-tuning using feedback (rewards) to optimize performance on specific tasks or align with human preferences.\n",
        "\n",
        " *The DeepSeek-R1 paper introduces DeepSeek-R1-Zero, which skips the SFT step and applies large-scale RL directly to a pre-trained base model (DeepSeek-V3-Base).*\n",
        "\n",
        " The paper shows that this pure RL approach can incentivize the model to develop impressive reasoning capabilities on its own, driven by the reward signal.\n",
        "\n",
        " **II.Key aspects of DeepSeek-R1-Zero's RL approach (from Section 2.2)**:\n",
        "\n",
        " **1.Pure RL**: **No initial SFT phase**.\n",
        " Rule-Based Rewards: Using simple, verifiable rules (like correctness of a final answer or adherence to a specific output format) to provide feedback, rather than complex learned reward models.Training Template: Guiding the model's output structure (e.g., including a thinking process before the final answer).\n",
        "\n",
        " **Self-Evolution**: The model naturally learns to improve its reasoning process over iterations to maximize rewards.\n",
        "\n",
        " Let's simulate these ideas with a simple example using Gini impurity.\n",
        "\n",
        " Simplified Example: Calculating Gini ImpurityOur simplified task will be calculating the Gini impurity for a small dataset split.Gini\n",
        "\n",
        " Impurity is calculated as **1−∑i=1C​(pi​)^2**, where pi​ is the proportion of instances belonging to class i in the subset, and C is the number of classes.\n",
        "\n",
        " Consider a node in a decision tree that has been split, resulting in a subset of data with the following class distribution:\n",
        "\n",
        " Class A: 4 instancesClass B: 6 instancesTotal instances = 10.Proportion of Class A (pA​) = 4/10=0.4\n",
        "\n",
        " Proportion of Class B (pB​) = 6/10=0.6\n",
        " Gini Impurity = 1−(pA​)2−(pB​)^2\n",
        " =1−(0.4)2−(0.6)2\n",
        " =1−0.16−0.36\n",
        " =1−0.52=0.48.\n",
        "\n",
        " So, the correct answer is 0.48.Our simplified \"model\" will be a function that tries to generate a response following a specific template.\n",
        "\n",
        " Our rule-based reward function will evaluate the response based on format and correctness of the calculated Gini impurity.\n",
        "\n",
        " The desired template is similar to the one used in DeepSeek-R1-Zero (Table 1 in the paper):\n",
        "\n",
        " <think> [reasoning process] </think> <answer> [final answer] </answer>\n",
        "\n",
        " Let's define the components in Python."
      ],
      "metadata": {
        "id": "OBOIQ5LVsz2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Introduction to the simplified RL Example in Python\n",
        "\n",
        "In this notebook we simulate the core ideas from Section 2.2 of the *DeepSeek-R1* paper, demonstrating how **pure reinforcement learning** (RL) with **rule-based rewards** can drive a base model to improve its reasoning over time. We choose a very simple task—calculating the Gini impurity of a two-class split—to focus on the RL mechanics rather than on large-scale model details.\n",
        "\n",
        "### 1. Gini Impurity Calculation\n",
        "```python\n",
        "def gini_impurity(counts):\n",
        "    total = sum(counts.values())\n",
        "    return 1 - sum((count / total) ** 2 for count in counts.values())\n",
        "````\n",
        "\n",
        "* **Definition**\n",
        "  Gini impurity measures how often a randomly chosen element from a set would be misclassified if labeled according to the class distribution.\n",
        "* **Formula**\n",
        "\n",
        "  $$\n",
        "    G = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "  $$\n",
        "\n",
        "  where $p_i$ is the proportion of class *i* in the node.\n",
        "* **Usage**\n",
        "  We compute the “true” Gini for our toy split (4 of A, 6 of B) as\n",
        "\n",
        "  $$\n",
        "    1 - (0.4^2 + 0.6^2) = 0.48.\n",
        "  $$\n",
        "\n",
        "### 2. SimpleModel: A Noisy “Policy”\n",
        "\n",
        "```python\n",
        "class SimpleModel:\n",
        "    def __init__(self, noise_level=0.5):\n",
        "        self.noise_level = noise_level\n",
        "    def generate_response(self, counts):\n",
        "        # build a <think>…</think> reasoning string\n",
        "        # add uniform noise in [−noise_level, +noise_level]\n",
        "        # round and wrap in <answer>…</answer>\n",
        "    def update(self, reward):\n",
        "        # reduce noise_level by a factor proportional to the reward\n",
        "```\n",
        "\n",
        "* **Analogy to an LLM**\n",
        "  The model’s `generate_response` stands in for a large language model composing a chain-of-thought:\n",
        "\n",
        "  ```\n",
        "  <think>… reasoning …</think> <answer>… value …</answer>\n",
        "  ```\n",
        "* **Noise parameter**\n",
        "  Controls how far the model’s answer can deviate from the true Gini.\n",
        "* **Update rule**\n",
        "  Whenever the model receives reward, it **reduces** its noise, thereby making future answers more precise.\n",
        "\n",
        "### 3. Rule-Based Reward Function\n",
        "\n",
        "```python\n",
        "def reward_fn(response, true_value, tol=0.1):\n",
        "    # 1) Check that the model used the exact <think>/<answer> template.\n",
        "    # 2) Extract the numeric answer.\n",
        "    # 3) If the answer is within ±tol, return (1 − error/tol), else 0.\n",
        "```\n",
        "\n",
        "* **Template enforcement**\n",
        "  Ensures the model has learned to include a reasoning block and a final answer block.\n",
        "* **Continuous reward shaping**\n",
        "  Partial credit for near-correct:\n",
        "\n",
        "  $$\n",
        "    \\text{reward} = \\max\\bigl(0,\\;1 - \\tfrac{|\\,\\hat G - G\\,|}{\\text{tol}}\\bigr).\n",
        "  $$\n",
        "\n",
        "### 4. Training Loop\n",
        "\n",
        "```python\n",
        "model = SimpleModel(noise_level=0.5)\n",
        "for epoch in range(1, N+1):\n",
        "    response = model.generate_response(class_counts)\n",
        "    r = reward_fn(response, true_gini, tol=0.1)\n",
        "    model.update(r)\n",
        "    # (optional) log noise & reward every few epochs\n",
        "```\n",
        "\n",
        "* **Pure RL**\n",
        "  Notice there is **no supervised fine-tuning** step. We apply the reward function directly to the base model.\n",
        "* **Self-evolution**\n",
        "  Over many iterations, the model perks up its “chain of thought” format and steadily **reduces noise**, reflecting how DeepSeek-R1-Zero discovers improved reasoning purely from rule-based feedback.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Experiment Further\n",
        "\n",
        "1. **Adjust the tolerance** (`tol`) to make the reward harder or easier to attain.\n",
        "2. **Vary the learning rate** (noise reduction per reward point).\n",
        "3. **Increase the number of classes** or data splits to see how the template scales.\n",
        "4. **Log intermediate predictions** (or plot convergence curves) to visualize learning dynamics.\n",
        "\n",
        "This minimal example captures the essence of applying RL directly to a base language model with simple, verifiable rewards and a fixed output template—exactly as DeepSeek-R1-Zero does at scale.\n"
      ],
      "metadata": {
        "id": "kHJRfCfqwwuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Task parameters\n",
        "class_counts = {'A': 4, 'B': 6}\n",
        "true_gini = 1 - sum((count / sum(class_counts.values())) ** 2 for count in class_counts.values())\n",
        "\n",
        "def gini_impurity(counts):\n",
        "    total = sum(counts.values())\n",
        "    return 1 - sum((count / total) ** 2 for count in counts.values())\n",
        "\n",
        "class SimpleModel:\n",
        "    def __init__(self, noise_level=0.5):\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def generate_response(self, counts):\n",
        "        reasoning = (f\"Node has counts {counts}. Total = {sum(counts.values())}. \"\n",
        "                     \"Proportions: \" +\n",
        "                     \", \".join(f\"{cls}={count/sum(counts.values()):.2f}\"\n",
        "                               for cls, count in counts.items()))\n",
        "        base = gini_impurity(counts)\n",
        "        noise = random.uniform(-self.noise_level, self.noise_level)\n",
        "        answer = round(base + noise, 2)\n",
        "        return f\"<think>{reasoning}</think> <answer>{answer}</answer>\"\n",
        "\n",
        "    def update(self, reward):\n",
        "        lr = 0.1\n",
        "        self.noise_level *= (1 - lr * reward)\n",
        "        self.noise_level = max(0, min(1, self.noise_level))\n",
        "\n",
        "def reward_fn(response, true_value, tol=0.1):\n",
        "    if not (response.startswith(\"<think>\") and \"</think>\" in response and\n",
        "            \"<answer>\" in response and response.endswith(\"</answer>\")):\n",
        "        return 0.0\n",
        "\n",
        "    match = re.search(r\"<answer>([-+]?\\d*\\.\\d+|\\d+)</answer>$\", response)\n",
        "    if not match:\n",
        "        return 0.0\n",
        "    predicted = float(match.group(1))\n",
        "\n",
        "    error = abs(predicted - true_value)\n",
        "    if error > tol:\n",
        "        return 0.0\n",
        "    return 1 - (error / tol)\n",
        "\n",
        "# Initialize model\n",
        "model = SimpleModel(noise_level=0.5)\n",
        "print(f\"True Gini impurity: {true_gini:.2f}\")\n",
        "\n",
        "# Show initial response before any updates\n",
        "initial_response = model.generate_response(class_counts)\n",
        "match = re.search(r\"<answer>([-+]?\\d*\\.\\d+|\\d+)</answer>$\", initial_response)\n",
        "initial_pred = float(match.group(1)) if match else None\n",
        "print(f\"Initial model response: {initial_response}\")\n",
        "print(f\"Initial predicted value: {initial_pred:.2f}\\n\")\n",
        "\n",
        "# Training loop summary\n",
        "for epoch in range(1, 51):\n",
        "    response = model.generate_response(class_counts)\n",
        "    reward = reward_fn(response, true_gini, tol=0.1)\n",
        "    model.update(reward)\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:2d} | Noise: {model.noise_level:.3f} | Last reward: {reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn6uNGpEvlid",
        "outputId": "f8b68e69-d0d0-429a-c5be-f28ebf0d54e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Gini impurity: 0.48\n",
            "Initial model response: <think>Node has counts {'A': 4, 'B': 6}. Total = 10. Proportions: A=0.40, B=0.60</think> <answer>0.62</answer>\n",
            "Initial predicted value: 0.62\n",
            "\n",
            "Epoch 10 | Noise: 0.490 | Last reward: 0.00\n",
            "Epoch 20 | Noise: 0.407 | Last reward: 0.00\n",
            "Epoch 30 | Noise: 0.398 | Last reward: 0.00\n",
            "Epoch 40 | Noise: 0.321 | Last reward: 0.00\n",
            "Epoch 50 | Noise: 0.308 | Last reward: 0.20\n"
          ]
        }
      ]
    }
  ]
}