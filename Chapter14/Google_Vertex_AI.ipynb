{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYvzs7cLR9oY"
      },
      "source": [
        "# Vertex AI\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "Google, as other editors advancing at full speed with LLM generative transformer models, is continually updating the available models.\n",
        "\n",
        "The models versions may not be stable. If you encounter an issue, go to Google's model versioning page to see which use.\n",
        "\n",
        "https://cloud.google.com/vertex-ai/docs/generative-ai/learn/model-versioning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VRWWL_vpNZpg"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoDSC70YVXog",
        "outputId": "a1cbe9e9-65fc-44cf-acf8-a0749aa411ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.42.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.12.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.60.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
            "Requirement already satisfied: numpy<2,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZwmwszyXSr9l"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()\n",
        "\n",
        "import vertexai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnCHBV8LJhmq"
      },
      "source": [
        "# Question Answering(QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_6EDI6AJjfT",
        "outputId": "1772b725-6f7f-4876-f176-f3ac5f9b8e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Gemini does not repeat existing\n",
            "text. It is a multimodal LLM, which\n",
            "means that it can understand and\n",
            "generate text, code, images, and more.\n",
            "It excels at reasoning across vast\n",
            "amounts of information, solving complex\n",
            "problems, and adapting to various tasks\n",
            "through fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"\"\"Answer a question on the text submitted\"\"\",\n",
        ")\n",
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"\"\"Answer a question on the text submitted\"\"\",\n",
        ")\n",
        "response = chat.send_message(\"\"\"Based on the following text, Does Gemini repeat existing text? “\n",
        "Gemini is a multimodal LLM, understanding and generating text, code, images, and more. It excels at reasoning across vast amounts of information, solving complex problems, and adapting to various tasks through fine-tuning.”\"\"\", **parameters)\n",
        "\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "\n",
        "import vertexai\n",
        "from vertexai.preview.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "\n",
        "def generate():\n",
        "  vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "  model = GenerativeModel(\"gemini-1.0-pro-001\")\n",
        "  responses = model.generate_content(\n",
        "    \"\"\"Based on the following text, does Gemini just repeat text: Gemini: Google\\'s Multimodal AI Powerhouse\n",
        "Gemini is Google\\'s most advanced and versatile AI model yet, representing a significant leap forward in artificial intelligence capabilities. Here\\'s a breakdown of its key aspects:\n",
        "Capabilities:\n",
        "Multimodal: Unlike most AI models that specialize in one type of data (text, image, code, etc.), Gemini is natively multimodal. This means it can understand, process, and combine different information formats seamlessly. This allows it to reason and solve problems in ways that were previously impossible for AI.\n",
        "State-of-the-art performance: Across various domains, Gemini demonstrates cutting-edge capabilities in tasks like question answering, code generation, image description, and more.\n",
        "Long-context understanding: Gemini 1.5, the latest iteration, boasts a breakthrough feature: understanding and reasoning with exceptionally long stretches of information. This enables it to grasp complex contexts and nuances in a way that surpasses previous models.\n",
        "Applications:\n",
        "Scientific discovery: By analyzing vast amounts of scientific data from various sources, Gemini can assist researchers in uncovering new insights and making groundbreaking discoveries.\n",
        "Product development: Its ability to understand and generate different creative formats makes Gemini valuable for product design, marketing, and content creation.\n",
        "Personalized experiences: By tailoring its responses to individual user preferences and contexts, Gemini can power personalized experiences in education, healthcare, and other domains.\n",
        "Technical underpinnings:\n",
        "Transformer architecture: At its core, Gemini leverages the power of transformer-based neural networks. These models excel at identifying complex relationships within data, allowing Gemini to understand and process information in a nuanced way.\n",
        "Multimodal pre-training: Unlike traditional models trained on single data types, Gemini undergoes multimodal pre-training. This means it\\'s exposed to various formats from the very beginning, enabling it to develop a comprehensive understanding of the world from the outset.\n",
        "Fine-tuning: Further refinement is achieved through fine-tuning on specific tasks and datasets. This allows Gemini to specialize in particular domains and excel at specific applications.\n",
        "Overall, Gemini represents a significant step forward in AI, paving the way for a future where machines can interact with the world in a more comprehensive and human-like manner.\n",
        "It\\'s important to note that I am also part of the Gemini family of AI models, specifically the 1.0 Pro version. While I share the core capabilities of understanding and generating text, I am still under development and learning new skills every day.\"\"\",\n",
        "    generation_config={\n",
        "        \"max_output_tokens\": 2048,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_p\": 1\n",
        "    },\n",
        "    safety_settings={\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "          generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "    },\n",
        "    stream=True,\n",
        "  )\n",
        "\n",
        "  for response in responses:\n",
        "    print(response.text, end=\"\")\n",
        "\n",
        "\n",
        "generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTWGk9On59jm",
        "outputId": "24a86cbc-435a-4558-e51d-664a7c38553a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, Gemini does not just repeat text. It is a multimodal AI model that can understand, process, and combine different information formats seamlessly. This allows it to reason and solve problems in ways that were previously impossible for AI."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9lPXAJShrC"
      },
      "source": [
        "# Question Answering(QA) - general"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvfFMN5ySyd3",
        "outputId": "fea040e0-97bf-4b8c-ddd6-5da7b704c293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am powered by PaLM 2, which stands for\n",
            "Pathways Language Model 2, a large\n",
            "language model from Google AI.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "def predict_large_language_model_sample(\n",
        "    project_id: str,\n",
        "    model_name: str,\n",
        "    temperature: float,\n",
        "    max_output_tokens: int,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    location: str = \"us-central1\",\n",
        "    ) :\n",
        "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "\n",
        "    chat_model = ChatModel.from_pretrained(model_name)\n",
        "    parameters = {\n",
        "      \"temperature\": temperature,\n",
        "      \"max_output_tokens\": max_output_tokens,\n",
        "      \"top_p\": top_p,\n",
        "      \"top_k\": top_k,\n",
        "    }\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "      examples=[]\n",
        "    )\n",
        "    response=chat.send_message('''What Transformer model are you using for this conversation?''',**parameters)\n",
        "    #print(response.text)\n",
        "    wrapped_text=textwrap.fill(response.text, width=40)\n",
        "    print(wrapped_text)\n",
        "\n",
        "\n",
        "predict_large_language_model_sample(\"aiex-57523\", \"chat-bison@001\", 0.2, 256, 0.8, 40, \"us-central1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Iggtu8SrI2"
      },
      "source": [
        "# Summarization of a conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJoN4TsxnjK6",
        "outputId": "209c481c-ee45-4798-be51-7f23b397456f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The customer wants to change the\n",
            "shipping address for an order. The\n",
            "service rep informs them that they can\n",
            "only do this if the order has not been\n",
            "fulfilled from the warehouse yet. Since\n",
            "the order has already shipped, the\n",
            "customer will need to contact the\n",
            "shipping provider to update their\n",
            "delivery details.\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Summarize the following conversation between a service rep and a customer in a few sentences. Use only the information from the conversation.\n",
        "\n",
        "Service Rep: How may I assist you today?\n",
        "Customer: I need to change the shipping address for an order.\n",
        "Service Rep: Ok, I can help you with that if the order has not been fulfilled from our warehouse yet. But if it has already shipped, then you will need to contact the shipping provider. Do you have the order ID?\n",
        "Customer: Yes, it\\'s 88986367.\n",
        "Service Rep: One minute please while I pull up your order information.\n",
        "Customer: No problem\n",
        "Service Rep: Ok, it looks like your order was shipped from our warehouse 2 days ago. It is now in the hands of  the shipping provider, so you will need to contact them to update your delivery details. You can track your order with the shipping provider here: https://www.shippingprovider.com\n",
        "Customer: Sigh, ok.\n",
        "Service Rep: Is there anything else I can help you with today?\n",
        "Customer: No, thanks.\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyaLZEkoS4W_"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQgm5sNDnxOD",
        "outputId": "c2ff636d-3563-440a-cb97-92a34462ae79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Is the sentiment positive or negative towards Louis van Gaal based on the article:\n",
        "\n",
        "Article:\n",
        "Louis van Gaal said he had no option but to substitute Paddy McNair in the first half against Southampton because the defender\\'s \\'confidence\\' was shot - but believes that it will benefit the youngster in the long run.\n",
        "The 19-year-old was hooked by Van Gaal after only 39 minutes at St Mary\\'s Stadium on Monday night during Manchester United\\'s 2-1 victory over the Saints.\n",
        "McNair was struggling to contain Southampton strikers Shane Long and Graziano Pelle, forcing Van Gaal into replacing him prematurely.\n",
        "Speaking to Sky Sports after the match, Van Gaal explained: \\'He (McNair) hadn\\'t any confidence. He had already given three big chances away.\n",
        "\\'I had to (substitute him), it\\'s very disappointing for me and also for Paddy, but I had to because as a manager, I\\'m responsible to win.\n",
        "\\'And I think, after the change, we played a little better.\\'\n",
        "But in spite of the fact United won the game, McNair was exposed time after time in defence and was substituted - even though Chris Smalling had already departed early with an injury.\n",
        "Jonny Evans came on to replace Smalling, before McNair made way for midfielder Ander Herrera as Michael Carrick dropped back in to the centre of defence in Van Gaal\\'s 3-5-2 system.\n",
        "And, despite admitting it will be difficult for McNair to accept being replaced so early, Van Gaal insisted that it was a necessity which will serve the Northern Irishman well long term.\n",
        "Van Gaal continued: \\'Of course, it\\'s tough (for McNair), but it\\'s also in his best interests.\\'\n",
        "The victory moved United up to third in the Premier League - their highest position since they claimed the title in 2012-13 under Sir Alex Ferguson. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IeUFdFGWSAp",
        "outputId": "f534df75-bff2-4987-a677-80cf581dbcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment towards Louis van Gaal is\n",
            "positive.  The article is about Louis\n",
            "van Gaal's decision to substitute Paddy\n",
            "McNair in the first half against\n",
            "Southampton. The article says that Van\n",
            "Gaal had to make the substitution\n",
            "because McNair was struggling and that\n",
            "it was a necessary decision. The article\n",
            "also says that Van Gaal believes that\n",
            "the substitution will benefit McNair in\n",
            "the long run.  Overall, the article\n",
            "presents Van Gaal in a positive light.\n",
            "It says that he is a good manager who is\n",
            "responsible for winning games and that\n",
            "he is making decisions that are in the\n",
            "best interests of his players.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Is the sentiment positive or negative towards Louis van Gaal based on the article and explain why in detail:\n",
        "\n",
        "Article:\n",
        "Louis van Gaal said he had no option but to substitute Paddy McNair in the first half against Southampton because the defender\\'s \\'confidence\\' was shot - but believes that it will benefit the youngster in the long run.\n",
        "The 19-year-old was hooked by Van Gaal after only 39 minutes at St Mary\\'s Stadium on Monday night during Manchester United\\'s 2-1 victory over the Saints.\n",
        "McNair was struggling to contain Southampton strikers Shane Long and Graziano Pelle, forcing Van Gaal into replacing him prematurely.\n",
        "Speaking to Sky Sports after the match, Van Gaal explained: \\'He (McNair) hadn\\'t any confidence. He had already given three big chances away.\n",
        "\\'I had to (substitute him), it\\'s very disappointing for me and also for Paddy, but I had to because as a manager, I\\'m responsible to win.\n",
        "\\'And I think, after the change, we played a little better.\\'\n",
        "But in spite of the fact United won the game, McNair was exposed time after time in defence and was substituted - even though Chris Smalling had already departed early with an injury.\n",
        "Jonny Evans came on to replace Smalling, before McNair made way for midfielder Ander Herrera as Michael Carrick dropped back in to the centre of defence in Van Gaal\\'s 3-5-2 system.\n",
        "And, despite admitting it will be difficult for McNair to accept being replaced so early, Van Gaal insisted that it was a necessity which will serve the Northern Irishman well long term.\n",
        "Van Gaal continued: \\'Of course, it\\'s tough (for McNair), but it\\'s also in his best interests.\\'\n",
        "The victory moved United up to third in the Premier League - their highest position since they claimed the title in 2012-13 under Sir Alex Ferguson. \"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rilcm2bGS-sC"
      },
      "source": [
        "# Multi-choice problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfMQS2dqoBoF",
        "outputId": "732b508a-adc1-4a6e-fa39-5fb2cb69f4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "culture\n"
          ]
        }
      ],
      "source": [
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Multi-choice problem: What is the topic of this text?\n",
        "- entertainment\n",
        "- technology\n",
        "- politics\n",
        "- sports\n",
        "- business\n",
        "- health\n",
        "- fun\n",
        "- culture\n",
        "- science\n",
        "\n",
        "Text: Samba, is a name or prefix used for several rhythmic variants, such as samba urbano carioca (urban Carioca samba), samba de roda (sometimes also called rural samba), recognized as part of the Intangible Cultural Heritage of Humanity by UNESCO, amongst many other forms of Samba, mostly originated in the Rio de Janeiro and Bahia States. Samba is a broad term for many of the rhythms that compose the better known Brazilian music genres that originated in the Afro-Brazilian communities of Bahia in the late 19th century and early 20th century, having continued its development on the communities of Rio de Janeiro in the early 20th century. Having its roots in the Afro-Brazilian Candomblé, as well as other Afro-Brazilian and Indigenous folk traditions, such as the traditional Samba de Caboclo, it is considered one of the most important cultural phenomena in Brazil and one of the country\\'s symbols. Present in the Portuguese language at least since the 19th century, the word \\\"samba\\\" was originally used to designate a \\\"popular dance\\\". Over time, its meaning has been extended to a \\\"batuque-like circle dance\\\", a dance style, and also to a \\\"music genre\\\". This process of establishing itself as a musical genre began in the 1910s and it had its inaugural landmark in the song \\\"Pelo Telefone\\\", launched in 1917. Despite being identified by its creators, the public, and the Brazilian music industry as \\\"samba\\\", this pioneering style was much more connected from the rhythmic and instrumental point of view to maxixe than to samba itself.\n",
        "\n",
        "Samba was modernly structured as a musical genre only in the late 1920s from the neighborhood of Estácio and soon extended to Oswaldo Cruz and other parts of Rio through its commuter rail. Today synonymous with the rhythm of samba, this new samba brought innovations in rhythm, melody and also in thematic aspects. Its rhythmic change based on a new percussive instrumental pattern resulted in a more \\\"batucado\\\" and syncopated style – as opposed to the inaugural \\\"samba-maxixe\\\" – notably characterized by a faster tempo, longer notes and a characterized cadence far beyond the simple ones palms used so far. Also the \\\"Estácio paradigm\\\" innovated in the formatting of samba as a song, with its musical organization in first and second parts in both melody and lyrics. In this way, the sambistas of Estácio created, structured and redefined the urban Carioca samba as a genre in a modern and finished way. In this process of establishment as an urban and modern musical expression, the Carioca samba had the decisive role of samba schools, responsible for defining and legitimizing definitively the aesthetic bases of rhythm, and radio broadcasting, which greatly contributed to the diffusion and popularization of the genre and its song singers. Thus, samba has achieved major projection throughout Brazil and has become one of the main symbols of Brazilian national identity. Once criminalized and rejected for its Afro-Brazilian origins, and definitely working-class music in its mythic origins, the genre has also received support from members of the upper classes and the country\\'s cultural elite.\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "#print(f\"Response from Model: {response.text}\")\n",
        "wrapped_text=textwrap.fill(response.text, width=40)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsxGgCS5XjBv"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw2TkOsGXkiV",
        "outputId": "7dbc10b4-1769-4caf-bacf-de0f34349714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: Sure, here is the code for a function in Python that calculates a Fibonacci suite up to 10:\n",
            "\n",
            "```python\n",
            "def fibonacci(n):\n",
            "  \"\"\"\n",
            "  This function calculates the nth Fibonacci number.\n",
            "\n",
            "  Args:\n",
            "    n: The nth Fibonacci number to calculate.\n",
            "\n",
            "  Returns:\n",
            "    The nth Fibonacci number.\n",
            "  \"\"\"\n",
            "\n",
            "  # Initialize the first two Fibonacci numbers.\n",
            "  a = 0\n",
            "  b = 1\n",
            "\n",
            "  # Iterate through the Fibonacci sequence until we reach the nth number.\n",
            "  for i in range(n - 1):\n",
            "    # Calculate the next Fibonacci number.\n",
            "    c = a + b\n",
            "\n",
            "    # Update the previous two Fibonacci numbers.\n",
            "    a = b\n",
            "    b = c\n",
            "\n",
            "  # Return the nth Fibonacci number.\n",
            "  return c\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\"\"\"Write the code in Python for a function in Python that calculates a Fibonacci  suite up to 10:\"\"\", **parameters)\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmExDk96XqqJ"
      },
      "source": [
        "copy code into cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SHM1bskUXtGc"
      },
      "outputs": [],
      "source": [
        "def fibonacci(n):\n",
        "  \"\"\"\n",
        "  This function calculates the Fibonacci sequence up to the given number.\n",
        "\n",
        "  Args:\n",
        "    n: The number to calculate the Fibonacci sequence up to.\n",
        "\n",
        "  Returns:\n",
        "    The Fibonacci sequence up to the given number.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the sequence with 0 and 1.\n",
        "  a = 0\n",
        "  b = 1\n",
        "\n",
        "  # Iterate through the sequence, calculating each new number.\n",
        "  for i in range(n):\n",
        "    # Calculate the next number in the sequence.\n",
        "    c = a + b\n",
        "\n",
        "    # Update the sequence.\n",
        "    a = b\n",
        "    b = c\n",
        "\n",
        "  # Return the sequence.\n",
        "  return a, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZwHyuSHXyZr"
      },
      "source": [
        "call the function and run the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIZLkZvwXyAA",
        "outputId": "5b833a21-e2fb-4f0e-a48d-f5df4a85f008"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(55, 89)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "fibonacci(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsuEsPSsDd0k",
        "outputId": "c4a83f59-b87b-462d-ca89-357eaa4613b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Model: ```python\n",
            "from vertexai.preview.language_models import CodeChatModel\n",
            "\n",
            "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
            "chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
            "parameters = {\n",
            "    \"temperature\": 0.5,\n",
            "    \"max_output_tokens\": 1024\n",
            "}\n",
            "chat = chat_model.start_chat()\n",
            "response = chat.send_message(\"\"\"Write the code in Python for a function in Python that calculates a Fibonacci  suite up to 10:\"\"\", **parameters)\n",
            "print(f\"Response from Model: {response.text}\")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import CodeGenerationModel\n",
        "\n",
        "vertexai.init(project=\"aiex-57523\", location=\"us-central1\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 1024\n",
        "}\n",
        "model = CodeGenerationModel.from_pretrained(\"code-bison@001\")\n",
        "response = model.predict(\n",
        "    prefix = \"\"\"Correct the following code in Python that calculates a Fibonacci suite up to 10 numbers starting from 0 or 1. The code is inaccurate because it calculates 11 numbers instead of 10. Provide a corrected code:from vertexai.preview.language_models import CodeChatModel\n",
        "\n",
        "vertexai.init(project=\\\"aiex-57523\\\", location=\\\"us-central1\\\")\n",
        "chat_model = CodeChatModel.from_pretrained(\\\"codechat-bison@001\\\")\n",
        "parameters = {\n",
        "    \\\"temperature\\\": 0.5,\n",
        "    \\\"max_output_tokens\\\": 1024\n",
        "}\n",
        "chat = chat_model.start_chat()\n",
        "response = chat.send_message(\\\"\\\"\\\"Write the code in Python for a function in Python that calculates a Fibonacci  suite up to 10:\\\"\\\"\\\", **parameters)\n",
        "print(f\\\"Response from Model: {response.text}\\\")\"\"\",\n",
        "    **parameters\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSmbU3ztFk9v",
        "outputId": "cd5f0913-1f8c-44a9-dd26-4b62f783632f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ],
      "source": [
        "def fibonacci(n):\n",
        "  \"\"\"Calculates the nth Fibonacci number.\n",
        "\n",
        "  Args:\n",
        "    n: The nth Fibonacci number to calculate.\n",
        "\n",
        "  Returns:\n",
        "    The nth Fibonacci number.\n",
        "  \"\"\"\n",
        "  if n == 0:\n",
        "    return 0\n",
        "  elif n == 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return fibonacci(n - 1) + fibonacci(n - 2)\n",
        "\n",
        "\n",
        "print(fibonacci(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}