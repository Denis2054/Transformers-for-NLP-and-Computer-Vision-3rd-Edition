{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8CMRQEUz5v4"
      },
      "source": [
        "# Encoder-decoder Transformer\n",
        "\n",
        "Copyright 2023, Denis Rothman\n",
        "\n",
        "Generated by OpenAI GPT-4 through advanced prompt engineering\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2re7iEKZpRQ"
      },
      "source": [
        "#Library installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErcnTN3CZLmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61ce766-72fb-4a48-82f3-1d1946582ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ezh3pP8Ka9-",
        "outputId": "53f186b3-9aa2-485d-dfb5-108181e627ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOpoWCSrZUDh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgXsHkmyQFB2"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "import re\n",
        "from collections import Counter\n",
        "import time"
      ],
      "metadata": {
        "id": "7_9lIg3DuUTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcEiqjVQMkZB",
        "outputId": "8a51247b-2e3f-487e-9957-a51d41d0bb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vocab scraped: 16,680\n",
            "Total tokens scraped: 165,004\n",
            "Number of batches: 5141\n",
            "cuda\n",
            "The encoder has 18,512,384 trainable parameters.\n",
            "The decoder has 33,903,408 trainable parameters.\n",
            "The total model has 52,415,792 trainable parameters.\n",
            "Epoch: 0,Batch: 1, Loss: 10.487588882446289\n",
            "Epoch: 0,Batch: 11, Loss: 6.941403388977051\n",
            "Epoch: 0,Batch: 21, Loss: 6.688475608825684\n",
            "Epoch: 0,Batch: 31, Loss: 6.32985782623291\n",
            "Epoch: 0,Batch: 41, Loss: 6.070192337036133\n",
            "Epoch: 0,Batch: 51, Loss: 5.888068199157715\n",
            "Epoch: 0,Batch: 61, Loss: 5.668067932128906\n",
            "Epoch: 0,Batch: 71, Loss: 5.494823455810547\n",
            "Epoch: 0,Batch: 81, Loss: 5.295373439788818\n",
            "Epoch: 0,Batch: 91, Loss: 5.165517807006836\n",
            "Epoch: 0,Batch: 101, Loss: 4.8827080726623535\n",
            "Epoch: 0,Batch: 111, Loss: 4.8755950927734375\n",
            "Epoch: 0,Batch: 121, Loss: 4.72114896774292\n",
            "Epoch: 0,Batch: 131, Loss: 4.641555309295654\n",
            "Epoch: 0,Batch: 141, Loss: 4.641575813293457\n",
            "Epoch: 0,Batch: 151, Loss: 4.432111740112305\n",
            "Epoch: 0,Batch: 161, Loss: 4.37265682220459\n",
            "Epoch: 0,Batch: 171, Loss: 4.433326721191406\n",
            "Epoch: 0,Batch: 181, Loss: 4.305866718292236\n",
            "Epoch: 0,Batch: 191, Loss: 4.188398361206055\n",
            "Epoch: 0,Batch: 201, Loss: 4.146466255187988\n",
            "Epoch: 0,Batch: 211, Loss: 4.057660102844238\n",
            "Raw logits: tensor([[-3.0776, -3.2959, -4.2214,  ..., -4.5057,  4.6195, -5.1582],\n",
            "        [-2.0785, -4.9279, -1.7135,  ..., -4.6819,  4.8313, -3.7842],\n",
            "        [-1.2048, -3.1399, -3.3544,  ..., -5.0516,  5.7755, -4.5781],\n",
            "        ...,\n",
            "        [-4.7630, -2.6499, -3.4628,  ..., -5.4312,  5.6215, -5.7124],\n",
            "        [-3.3234, -3.8039, -3.9461,  ..., -5.9711,  4.8056, -5.3998],\n",
            "        [-4.1223, -4.4564, -3.3576,  ..., -5.3494,  4.8003, -4.9385]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 0, Loss: 3.9984071254730225\n",
            "Epoch: 1,Batch: 1, Loss: 4.111127853393555\n",
            "Epoch: 1,Batch: 11, Loss: 4.041340351104736\n",
            "Raw logits: tensor([[-3.3606, -3.1754, -3.9481,  ..., -6.2432,  5.0114, -5.8827],\n",
            "        [-3.3013, -4.7378, -2.3366,  ..., -5.6853,  5.6847, -5.6650],\n",
            "        [-2.2740, -2.4257, -5.2361,  ..., -5.4129,  3.8164, -6.1101],\n",
            "        ...,\n",
            "        [-2.7701, -4.4335, -3.4175,  ..., -5.9921,  5.8410, -5.8716],\n",
            "        [-3.8166, -4.5042, -4.3357,  ..., -4.6697,  4.9990, -5.4253],\n",
            "        [-3.5585, -2.4212, -3.6803,  ..., -4.7825,  5.4505, -4.5182]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 1, Loss: 3.998396396636963\n",
            "Epoch: 2,Batch: 1, Loss: 4.052567958831787\n",
            "Raw logits: tensor([[-4.3808, -1.0159, -3.4376,  ..., -3.5844,  4.1051, -5.3158],\n",
            "        [-3.8158, -2.6567, -4.6177,  ..., -5.9109,  6.7757, -6.4997],\n",
            "        [-2.4229, -3.2735, -3.4028,  ..., -4.7263,  5.8893, -4.4364],\n",
            "        ...,\n",
            "        [-2.1127, -3.4978, -2.3166,  ..., -4.0565,  3.6696, -5.6862],\n",
            "        [-3.3784, -0.9228, -2.8873,  ..., -3.7071,  5.1112, -3.5848],\n",
            "        [-3.6374, -2.5210, -3.1514,  ..., -6.2557,  5.4745, -4.7379]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 2, Loss: 3.9592511653900146\n",
            "Epoch: 3,Batch: 1, Loss: 3.9833385944366455\n",
            "Raw logits: tensor([[-2.2652, -2.7380, -0.0187,  ..., -4.7589,  5.8706, -5.4020],\n",
            "        [-3.1987, -3.2019, -3.5973,  ..., -4.9353,  5.2436, -4.8572],\n",
            "        [-3.2829, -3.9870, -4.4056,  ..., -4.8873,  4.9307, -4.1062],\n",
            "        ...,\n",
            "        [-3.6524, -0.8552, -2.1265,  ..., -4.1339,  5.0953, -4.8233],\n",
            "        [-2.9483, -2.9884, -3.4539,  ..., -3.8261,  5.6116, -3.8893],\n",
            "        [-4.3338, -3.5186, -5.0445,  ..., -5.2962,  5.2536, -5.7831]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 3, Loss: 3.9833385944366455\n",
            "Epoch: 4,Batch: 1, Loss: 3.9794795513153076\n",
            "Raw logits: tensor([[-3.8157, -1.4016, -4.4807,  ..., -4.5476,  5.0659, -5.1180],\n",
            "        [-1.1563, -1.8286,  5.5136,  ..., -2.7851,  6.7253, -3.0234],\n",
            "        [-0.5139, -1.3812, -0.7356,  ..., -4.2859,  6.1077, -3.1822],\n",
            "        ...,\n",
            "        [-3.8916, -3.6119, -1.8960,  ..., -4.3265,  4.8095, -4.6319],\n",
            "        [-2.0517, -3.3008, -2.7523,  ..., -4.8385,  4.5578, -4.3400],\n",
            "        [-4.0249, -2.4637, -2.7928,  ..., -6.0385,  6.3961, -6.2655]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 4, Loss: 3.9794795513153076\n",
            "Epoch: 5,Batch: 1, Loss: 4.020692348480225\n",
            "Raw logits: tensor([[-3.3715, -4.0305, -3.2741,  ..., -4.9965,  5.0233, -5.3011],\n",
            "        [-4.5378, -1.2778, -4.9376,  ..., -4.9103,  5.6580, -6.2125],\n",
            "        [-1.3449, -1.8018,  5.4129,  ..., -2.9541,  6.6418, -2.9514],\n",
            "        ...,\n",
            "        [-3.4886, -3.4694, -1.7059,  ..., -4.5427,  5.6371, -5.6200],\n",
            "        [-3.1839, -1.2245, -5.4344,  ..., -5.4768,  5.3881, -5.7026],\n",
            "        [-3.6229, -3.3226, -3.9723,  ..., -4.1378,  5.9427, -4.2635]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 5, Loss: 3.9393465518951416\n",
            "Epoch: 6,Batch: 1, Loss: 4.009223461151123\n",
            "Raw logits: tensor([[-4.0332, -2.5687, -4.0148,  ..., -6.1939,  5.6017, -5.6231],\n",
            "        [-1.4460, -1.9708,  4.9428,  ..., -2.8001,  6.9232, -3.9718],\n",
            "        [-4.2341, -2.7794, -1.7823,  ..., -5.5291,  5.4545, -4.9584],\n",
            "        ...,\n",
            "        [-3.3012, -3.1108, -4.1535,  ..., -4.1323,  5.2737, -4.4376],\n",
            "        [-3.7713, -2.4051, -2.9343,  ..., -4.5014,  6.8582, -4.2168],\n",
            "        [-3.6338, -5.0429, -3.6247,  ..., -4.3519,  5.1075, -3.9274]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 6, Loss: 3.971320629119873\n",
            "Epoch: 7,Batch: 1, Loss: 3.9943158626556396\n",
            "Raw logits: tensor([[-4.6799, -4.4588, -4.0834,  ..., -4.5874,  5.5398, -5.8404],\n",
            "        [-3.5943, -4.1768, -3.4854,  ..., -4.9203,  4.9235, -5.1389],\n",
            "        [-2.6639, -3.7159, -1.7361,  ..., -5.0649,  4.7091, -4.4592],\n",
            "        ...,\n",
            "        [-1.6857, -1.9935,  4.7112,  ..., -3.3397,  7.2950, -3.2764],\n",
            "        [-2.4084, -2.3027, -1.1957,  ..., -2.7227,  5.9601, -3.9973],\n",
            "        [-3.8922, -0.0744, -3.6211,  ..., -3.5214,  4.2036, -3.6972]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 7, Loss: 3.9943158626556396\n",
            "Epoch: 8,Batch: 1, Loss: 3.9515810012817383\n",
            "Raw logits: tensor([[-4.5558, -1.9202, -2.9259,  ..., -6.3902,  6.4055, -6.1263],\n",
            "        [-1.1277, -2.1382,  4.9689,  ..., -2.9701,  6.7107, -2.9199],\n",
            "        [-2.9575, -2.5096, -4.3999,  ..., -4.7077,  4.4477, -5.9542],\n",
            "        ...,\n",
            "        [-3.6080,  0.0385, -4.4362,  ..., -4.1856,  5.2719, -4.1334],\n",
            "        [-2.5060, -3.9905, -3.7421,  ..., -5.7959,  5.1623, -5.4267],\n",
            "        [-2.8972, -1.6336, -2.4319,  ..., -4.9785,  5.3802, -3.7398]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 8, Loss: 3.9515810012817383\n",
            "Epoch: 9,Batch: 1, Loss: 3.9519684314727783\n",
            "Raw logits: tensor([[-3.1399, -4.8330, -2.8578,  ..., -4.5486,  5.1709, -4.3621],\n",
            "        [-2.4601, -3.3429, -3.7939,  ..., -4.0243,  4.5046, -5.0417],\n",
            "        [-2.9720, -1.4108, -2.3593,  ..., -2.0825,  4.2209, -3.0397],\n",
            "        ...,\n",
            "        [-4.0678, -4.4255, -3.5920,  ..., -6.0777,  5.3736, -5.7802],\n",
            "        [-3.9016, -4.1995, -2.8734,  ..., -5.2501,  4.6336, -4.2495],\n",
            "        [-4.2460, -2.3315, -2.4376,  ..., -5.1179,  5.9962, -4.1426]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 9, Loss: 3.9519684314727783\n",
            "Epoch: 10,Batch: 1, Loss: 3.9945297241210938\n",
            "Raw logits: tensor([[-2.8832, -3.4698, -2.9165,  ..., -4.4478,  5.4518, -4.4400],\n",
            "        [-3.2547, -1.5781, -3.3856,  ..., -4.5816,  4.8569, -4.5228],\n",
            "        [-2.8624, -0.9426, -3.8970,  ..., -6.0510,  5.5450, -6.3033],\n",
            "        ...,\n",
            "        [-3.2908, -4.3109, -2.8561,  ..., -4.4729,  4.9694, -5.3869],\n",
            "        [-5.0849, -2.6877, -2.6906,  ..., -4.1987,  5.8073, -4.4624],\n",
            "        [-1.3649, -2.0952,  4.7716,  ..., -3.1960,  7.2151, -3.1638]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 10, Loss: 3.9945297241210938\n",
            "Epoch: 11,Batch: 1, Loss: 3.9850943088531494\n",
            "Raw logits: tensor([[-3.1210, -4.4965, -3.2277,  ..., -5.2243,  5.2724, -4.9812],\n",
            "        [-3.4821, -1.9449, -3.9141,  ..., -5.4462,  5.8591, -4.6348],\n",
            "        [-3.4708, -3.7892, -4.1003,  ..., -3.7565,  4.4673, -5.6498],\n",
            "        ...,\n",
            "        [-3.4570, -3.1038, -4.1249,  ..., -5.1184,  4.7180, -6.3513],\n",
            "        [-3.8484, -3.3163, -3.8917,  ..., -5.7119,  4.8821, -5.1008],\n",
            "        [-3.9664, -3.0639, -2.8479,  ..., -4.9416,  5.7755, -4.5605]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 11, Loss: 3.9850943088531494\n",
            "Epoch: 12,Batch: 1, Loss: 4.033105373382568\n",
            "Raw logits: tensor([[-3.0635, -3.2933, -4.8061,  ..., -6.0763,  5.4310, -6.3515],\n",
            "        [-2.5938, -4.2216, -3.4597,  ..., -5.3078,  4.9750, -5.5397],\n",
            "        [-3.1752, -2.9540, -3.4732,  ..., -4.1648,  3.9607, -5.1479],\n",
            "        ...,\n",
            "        [-4.7346, -2.7632, -3.8305,  ..., -5.2942,  6.1689, -5.9407],\n",
            "        [-2.9480, -3.1271, -4.6820,  ..., -5.7791,  4.7706, -6.9065],\n",
            "        [-2.7149, -4.0572, -3.4641,  ..., -4.6051,  4.7309, -5.1692]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 12, Loss: 3.9373044967651367\n",
            "Epoch: 13,Batch: 1, Loss: 4.0263671875\n",
            "Raw logits: tensor([[-3.3583, -4.0844, -4.8933,  ..., -5.5350,  4.9783, -5.6060],\n",
            "        [-1.6536, -3.8982, -1.3693,  ..., -4.3598,  4.5405, -3.2345],\n",
            "        [-3.5182, -3.1354, -4.5200,  ..., -3.6604,  4.8580, -4.7005],\n",
            "        ...,\n",
            "        [-3.5880, -2.6563, -4.1526,  ..., -3.9760,  5.1454, -4.1066],\n",
            "        [-3.9213, -3.6160, -2.2824,  ..., -4.4930,  5.3393, -5.0376],\n",
            "        [-2.8438, -3.7550, -4.6419,  ..., -4.5692,  5.6846, -4.8870]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 13, Loss: 3.9568960666656494\n",
            "Epoch: 14,Batch: 1, Loss: 4.0078206062316895\n",
            "Raw logits: tensor([[-4.5451, -0.7894, -2.8204,  ..., -4.3616,  4.7795, -4.7262],\n",
            "        [-2.4898, -3.9921, -3.4342,  ..., -5.2723,  5.0348, -5.4027],\n",
            "        [-4.4953, -0.8120, -2.6758,  ..., -4.6888,  5.0934, -4.8911],\n",
            "        ...,\n",
            "        [-2.9413, -3.8342, -4.9754,  ..., -4.5326,  4.0961, -4.6497],\n",
            "        [-4.1343, -3.8506, -4.7691,  ..., -6.6064,  5.6570, -4.6502],\n",
            "        [-3.0657, -3.6656, -3.4708,  ..., -4.5441,  4.9070, -5.0410]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 14, Loss: 3.98024320602417\n",
            "Epoch: 15,Batch: 1, Loss: 3.9575133323669434\n",
            "Raw logits: tensor([[-2.1180, -1.8765, -4.6993,  ..., -3.8160,  4.2405, -4.9337],\n",
            "        [-3.7665, -1.8115, -3.4300,  ..., -5.3698,  6.1248, -5.0783],\n",
            "        [-1.1946, -2.1789,  4.4105,  ..., -3.0747,  6.6939, -3.1220],\n",
            "        ...,\n",
            "        [-2.6383, -1.2903, -3.5399,  ..., -4.4218,  3.6533, -4.2320],\n",
            "        [-4.1346, -2.3726, -3.5101,  ..., -4.7428,  5.9396, -5.3991],\n",
            "        [-3.0626, -3.8212, -3.2550,  ..., -4.9223,  5.7759, -4.3432]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 15, Loss: 3.9575133323669434\n",
            "Epoch: 16,Batch: 1, Loss: 4.020531177520752\n",
            "Raw logits: tensor([[-3.1741, -3.9747, -2.4898,  ..., -5.6340,  4.8956, -5.7297],\n",
            "        [-4.7798, -1.2541, -3.6984,  ..., -3.7698,  6.0522, -4.2306],\n",
            "        [-5.3972, -3.9362, -4.5796,  ..., -6.0600,  6.5430, -6.3287],\n",
            "        ...,\n",
            "        [-5.3914, -4.1887, -4.1426,  ..., -6.2555,  6.4201, -5.2233],\n",
            "        [-3.6210, -4.3520, -3.4128,  ..., -5.0818,  5.0670, -5.3198],\n",
            "        [-3.4605, -4.1854, -1.6928,  ..., -4.6003,  5.6824, -5.6692]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 16, Loss: 3.9580774307250977\n",
            "Epoch: 17,Batch: 1, Loss: 3.9374051094055176\n",
            "Raw logits: tensor([[-4.7664, -1.6171, -4.4419,  ..., -5.2771,  5.0874, -4.8044],\n",
            "        [-3.2483, -3.6616, -2.7000,  ..., -4.4097,  5.3106, -4.8397],\n",
            "        [-2.5532, -2.5999, -2.4630,  ..., -5.8745,  5.7548, -6.2320],\n",
            "        ...,\n",
            "        [-3.9572, -3.3241, -2.4717,  ..., -4.9561,  4.2271, -3.7458],\n",
            "        [-3.2021,  0.1179, -3.6845,  ..., -5.3664,  4.8780, -5.5806],\n",
            "        [-4.4849, -6.2425, -4.0782,  ..., -5.9050,  6.1594, -5.9637]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 17, Loss: 3.9374051094055176\n",
            "Epoch: 18,Batch: 1, Loss: 3.9862284660339355\n",
            "Raw logits: tensor([[-3.6752, -4.2981, -2.8833,  ..., -4.9009,  5.1006, -4.9856],\n",
            "        [-3.3528, -1.8779, -4.1067,  ..., -5.3905,  3.7566, -4.2810],\n",
            "        [-2.4195, -4.1053, -3.5190,  ..., -5.3354,  5.0146, -5.5336],\n",
            "        ...,\n",
            "        [-3.7627, -2.3882, -4.4244,  ..., -5.2216,  5.1276, -5.2713],\n",
            "        [-3.4236, -2.4695, -4.3064,  ..., -5.3797,  5.5493, -4.5342],\n",
            "        [-1.3413, -2.0612,  3.8035,  ..., -2.8255,  6.8835, -3.0263]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 18, Loss: 3.9862284660339355\n",
            "Epoch: 19,Batch: 1, Loss: 4.09940767288208\n",
            "Raw logits: tensor([[-3.7296, -1.0543, -3.9201,  ..., -4.3340,  4.7049, -3.4138],\n",
            "        [-2.3830, -3.5585, -0.3712,  ..., -4.5724,  6.4165, -5.3221],\n",
            "        [-3.7073, -5.1979, -3.3876,  ..., -5.5130,  5.5324, -4.1967],\n",
            "        ...,\n",
            "        [-2.1779, -2.8798, -0.9508,  ..., -4.5891,  6.6277, -4.8215],\n",
            "        [-4.1462, -3.9636, -3.5880,  ..., -5.5025,  5.4373, -4.8624],\n",
            "        [-3.7016, -2.2667, -2.0679,  ..., -5.0416,  4.9304, -4.4823]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "Epoch: 19, Loss: 3.936788558959961\n",
            "Time taken for training: 201.8519058227539 seconds\n"
          ]
        }
      ],
      "source": [
        "def create_vocab(text, vocab_size):\n",
        "    tokenized_text = nltk.word_tokenize(text)\n",
        "    word_freq = Counter(tokenized_text)\n",
        "    vocab = {word: i for i, (word, _) in enumerate(word_freq.most_common(vocab_size))}\n",
        "    return vocab\n",
        "\n",
        "def scrape_wikipedia(urls):\n",
        "    text = \"\"\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        for paragraph in paragraphs:\n",
        "            text += paragraph.get_text()\n",
        "    return text\n",
        "\n",
        "def create_dataset(vocab_size, input_seq_length, text):\n",
        "    dataset = []\n",
        "    tokens = word_tokenize(text)\n",
        "    vocab = {word: i for i, word in enumerate(set(tokens))}\n",
        "    for i in range(0, len(tokens) - input_seq_length):\n",
        "        input_sequence = [vocab[word] for word in tokens[i: i + input_seq_length]]\n",
        "        target = input_sequence[1:] + [vocab_size - 2]\n",
        "        dataset.append((torch.tensor(input_sequence), torch.tensor(target)))\n",
        "    return dataset, vocab, tokens  # returning dataset and vocabulary\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.data = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, h, d_ff, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, h)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.embedding(input)\n",
        "        x = self.dropout(x)\n",
        "        attn_output, _ = self.self_attention(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, h)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input, encoder_output, lookahead_mask, padding_mask, training):\n",
        "        x = self.embedding(input)\n",
        "        x = self.dropout(x)\n",
        "        attn_output1, _ = self.self_attention(x, x, x, attn_mask=lookahead_mask, key_padding_mask=padding_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output1))\n",
        "        if encoder_output is not None:\n",
        "            attn_output2, _ = self.self_attention(x, encoder_output, encoder_output)\n",
        "            x = self.norm2(x + self.dropout(attn_output2))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def main():\n",
        "    urls = [\n",
        "    'https://en.wikipedia.org/wiki/American_Revolution',\n",
        "    'https://en.wikipedia.org/wiki/American_Civil_War',\n",
        "    'https://en.wikipedia.org/wiki/World_War_I',\n",
        "    'https://en.wikipedia.org/wiki/World_War_II',\n",
        "    'https://en.wikipedia.org/wiki/Renaissance',\n",
        "    'https://en.wikipedia.org/wiki/Industrial_Revolution',\n",
        "    'https://en.wikipedia.org/wiki/French_Revolution',\n",
        "    'https://en.wikipedia.org/wiki/Ancient_Greece',\n",
        "    'https://en.wikipedia.org/wiki/Roman_Empire',\n",
        "    'https://en.wikipedia.org/wiki/Enlightenment'\n",
        "    ]\n",
        "    vocab_size = 30000\n",
        "    input_seq_length = 512\n",
        "    h = 8\n",
        "    d_k = 64\n",
        "    d_v = 64\n",
        "    d_model = 512\n",
        "    d_ff = 2048\n",
        "    dropout_rate = 0.1\n",
        "    epochs = 20\n",
        "    batch_size = 32\n",
        "    loss_threshold=4\n",
        "    showlogits=1\n",
        "    text = scrape_wikipedia(urls)\n",
        "    raw_dataset, vocab,tokens = create_dataset(vocab_size, input_seq_length, text)\n",
        "    total_words=len(vocab)\n",
        "    total_tokens=len(tokens)\n",
        "    print(f'Total vocab scraped: {total_words:,}')\n",
        "    print(f'Total tokens scraped: {total_tokens:,}')\n",
        "    torch.save(raw_dataset, \"raw_dataset.pt\")\n",
        "    dataset = TextDataset(raw_dataset)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    num_batches = len(data_loader)\n",
        "    print(f'Number of batches: {num_batches}') # input and target sentences\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    encoder = Encoder(vocab_size, d_model, h, d_ff, dropout_rate).to(device)\n",
        "    decoder = Decoder(vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, dropout_rate).to(device)\n",
        "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
        "\n",
        "    num_parameters_encoder = count_parameters(encoder)\n",
        "    num_parameters_decoder = count_parameters(decoder)\n",
        "\n",
        "    print(f'The encoder has {num_parameters_encoder:,} trainable parameters.')\n",
        "    print(f'The decoder has {num_parameters_decoder:,} trainable parameters.')\n",
        "    total_parameters = num_parameters_encoder + num_parameters_decoder\n",
        "    print(f'The total model has {total_parameters:,} trainable parameters.')\n",
        "\n",
        "    # Start time\n",
        "    start_time = time.time()\n",
        "    with open(\"loss.txt\", \"w\") as f:\n",
        "      for epoch in range(epochs):\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            inputs = inputs.to(device).long()  # Move inputs to device\n",
        "            targets = targets.to(device).long()  # Move targets to device\n",
        "            encoder_output = encoder(inputs)\n",
        "            output = decoder(inputs, encoder_output, None, None, training=True)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            loss = F.cross_entropy(output, targets)\n",
        "            # Print the batch number and loss every 100 steps\n",
        "            if i % 10 == 0:\n",
        "              print(f\"Epoch: {epoch},Batch: {i+1}, Loss: {loss.item()}\")\n",
        "            # Write the loss to the file\n",
        "            f.write(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\\n\")\n",
        "            if loss<loss_threshold :\n",
        "              # Printing the raw logits\n",
        "              print('Raw logits:', output)\n",
        "              break\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
        "    # End time\n",
        "    end_time = time.time()\n",
        "    print(\"Time taken for training: {} seconds\".format(end_time - start_time))\n",
        "    return encoder, decoder\n",
        "99\n",
        "encoder, decoder = main()\n",
        "\n",
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"decoder\": decoder.state_dict()\n",
        "}, \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total sequence pairs = (165,050 - 512)\n",
        "Number of batches = Total sequence pairs / 32\n",
        "                  â‰ˆ 5142"
      ],
      "metadata": {
        "id": "bIchhjDGrab6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet provided is constructing sequences from tokens and then preparing batches from these sequences. Here's a breakdown of what's happening:\n",
        "\n",
        "- You have a total of \\( 165,050 \\) tokens.\n",
        "- You're using sequences of \\( 512 \\) tokens to create input sequences.\n",
        "- For each input sequence, you have an associated target sequence, which is often a shifted version of the input sequence in text generation tasks.\n",
        "- Since you are using sequences of length \\( 512 \\), the last \\( 512 \\) tokens of your text won't have enough subsequent tokens to form a full sequence, so you subtract this value.\n",
        "- The total number of input-target pairs is \\( 165,050 - 512 \\), and these pairs are then divided into batches.\n",
        "\n",
        "The number of batches is then calculated by dividing the total number of input-target pairs by the batch size. The calculation only counts the input sequences, but it implicitly counts the corresponding target sequences since there is a one-to-one correspondence between input sequences and target sequences.\n",
        "\n",
        "In summary, the subtraction of \\( 512 \\) accounts for the fact that you're creating sequences of that length, and it ensures that you don't attempt to create a sequence that would extend beyond the end of your tokens. This subtraction doesn't specifically relate to the distinction between input and target sequences; instead, it relates to the sequence length you are using to construct both input and target sequences."
      ],
      "metadata": {
        "id": "Rx8mORfesfCy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1b5ltBZc1rh"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ita3o_pIppX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c898abac-d2a9-4894-ee9c-78e2967b28fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([ 6363, 11622,  9706, 15723,  1957,   426, 12534,  7721, 11662, 12576,\n",
            "         2818,  4281,  5817,  8019,   593, 14137, 12534, 10173,  2167,  4872,\n",
            "         8996,  9522, 11622,  9900,  9655, 16404, 15119,  9779,  5528,  8786,\n",
            "         7047, 10173,  4644,  8996,  6870, 15602, 10544, 12750,  8996, 11622,\n",
            "         9706, 16404,  8996,   488,  6266, 15046,  7379, 13757,  5528,  8996,\n",
            "         5817,  2089, 12534,  5744,  8996,  7458,  5413, 13182,  8996,  7587,\n",
            "        10444,  1154, 12353,  8504,  3759,  1578, 13447, 12750,  2307, 12534,\n",
            "         2732,  2112,  2167, 11622,  4909, 13782,  7047, 13400,  5316, 15576,\n",
            "         8996,  5817,  3262, 16404,  1149,   115,  4281, 15119, 11420, 11221,\n",
            "         1938, 10875,  5320,  2167, 13162,  7047,  8996, 14869, 16404,  5817,\n",
            "         3809,  4792,  4950,  8996,   443,  1149, 13755,  3289, 11717, 12750,\n",
            "         1254,  4281,  7379,  1477, 11655, 16404, 15119, 13551, 14436, 11971,\n",
            "        15576,  3809, 14059,  2167,  7718,  8996, 14869, 16404,  8085, 16404,\n",
            "         8996,  5817,  3262,  5793, 10350, 12576, 13551, 12857,  7047, 15311,\n",
            "         8996, 11622,   443,  2773,  5754, 10875,  5110, 15576,  8996,  5817,\n",
            "        13810, 12534,  6003,  8996,   252, 12750,  8996, 11622,   443,   808,\n",
            "         1662,  4281,  3209, 12576,  3554,  8996,  5817, 13810, 12534,  5549,\n",
            "         8996,   443,  1163,  7548,  3759,  4784,  2167,  6201, 10900,  4956,\n",
            "         4872, 14137, 16404,  8996,  5817,  3262,  5793,  8996, 14374,  6290,\n",
            "        16404, 15119, 13752,  8154,  3759,  7071,  5215, 16404, 12963, 16404,\n",
            "        12534, 11031,  3838, 12443,  4281,  8996,   443, 16404, 12632,  7047,\n",
            "         3809, 16104, 12534,  3911,  4281,  7713,  5528, 10408,   443,  2920,\n",
            "         8996, 14374,  6290, 16285,  4281,  3684,  5881,  5274,  7047, 16323,\n",
            "         1149,  5572,  2167,  6363,  5817,  1758,  8996, 14374,  6290, 16404,\n",
            "        13128,  3544,  7422,   757, 11420, 11582,  2581,  4281,  2253,   808,\n",
            "         3262, 14970, 11413, 12750,  8996, 11486, 13550, 16404,  1149,  3935,\n",
            "        12750, 14019,  8154, 12534, 11754, 13752,  3759,  8996, 11802,   443,\n",
            "         2167,  4872,  1957,  3259,  7047, 11835,  1149, 15603,  8751,  4281,\n",
            "         8996,   443, 16404, 15119, 15723,  1033, 10754,  4281,  8996, 12402,\n",
            "         2722, 12750,  5500,  5806, 16404,  8505,  1249,  2246,  5934,  4844,\n",
            "         7047,  6348, 16404,  3911,  4281,  8996,  6348,  1021,  3759,  5074,\n",
            "         5689, 16404,   377,  2167,  6363,  5817, 13179, 11551,  1758, 11031,\n",
            "        12750,  8996, 11486,  1608,  4281,   377, 16404,   757,  4784, 14378,\n",
            "         8849, 14448,  3759,  1410,  4281, 10759,  7047,  2825,  7529,  3262,\n",
            "        14970,  1786,  7047, 14448,  8996,   443,  2167,  6363, 11802,   443,\n",
            "        12366,   933, 16404,  7587,   428,  8996,  4151,  4281,  8059,  4815,\n",
            "         4281,  2317, 12534, 11551, 15110,  8996,  6348, 10805, 12409,  4281,\n",
            "         6348, 12259,  3759,  3001,  6889, 16404, 16218, 16404, 15119, 11294,\n",
            "         3612,  3544,  2167,  6363,  5817, 12366, 15576, 14750,  6348, 12259,\n",
            "        12534, 16479,  1149,  4386, 12750,  7842,  1513, 16404, 15119,  3523,\n",
            "         1465,  5500,  1163, 10179,  1254,  2167,  4872,  8361, 13178, 16404,\n",
            "         4281, 13859, 12750,  5500, 16404, 10609, 12750,  8996, 11802,   443,\n",
            "         1005, 15318,  7047,  7274, 16404,  9400, 11420,  6625,  8996,  4796,\n",
            "        16626, 16285, 12534, 14698, 15121, 11165,  7047,  1662, 14970,  3809,\n",
            "        10858,  2167, 10424, 12750,  1662, 13551,  4225, 13182,  1737, 11035,\n",
            "         5166,  9343,  1737, 14995,  5166, 16404, 12534,  4909,  8266, 14378,\n",
            "        13702,  7047,  8996,  2089, 13551,  4225, 13182,  1737, 16612,  5166,\n",
            "         9343,  1737,   179,  2167,  5166,  4872, 10763,  8786, 16404,  8996,\n",
            "         5817, 13810, 11138,  5500,  7047,  8501,  4281,  1149, 12353, 12750,\n",
            "        10327, 10781, 12534,  8751, 16404, 12534,  4784,  1005,  1957, 10759,\n",
            "         7047,  6804, 11622,   505,   749,  2167, 11622,  6567, 12189,  8114,\n",
            "        16404,  1149])\n",
            "Target: tensor([11622,  9706, 15723,  1957,   426, 12534,  7721, 11662, 12576,  2818,\n",
            "         4281,  5817,  8019,   593, 14137, 12534, 10173,  2167,  4872,  8996,\n",
            "         9522, 11622,  9900,  9655, 16404, 15119,  9779,  5528,  8786,  7047,\n",
            "        10173,  4644,  8996,  6870, 15602, 10544, 12750,  8996, 11622,  9706,\n",
            "        16404,  8996,   488,  6266, 15046,  7379, 13757,  5528,  8996,  5817,\n",
            "         2089, 12534,  5744,  8996,  7458,  5413, 13182,  8996,  7587, 10444,\n",
            "         1154, 12353,  8504,  3759,  1578, 13447, 12750,  2307, 12534,  2732,\n",
            "         2112,  2167, 11622,  4909, 13782,  7047, 13400,  5316, 15576,  8996,\n",
            "         5817,  3262, 16404,  1149,   115,  4281, 15119, 11420, 11221,  1938,\n",
            "        10875,  5320,  2167, 13162,  7047,  8996, 14869, 16404,  5817,  3809,\n",
            "         4792,  4950,  8996,   443,  1149, 13755,  3289, 11717, 12750,  1254,\n",
            "         4281,  7379,  1477, 11655, 16404, 15119, 13551, 14436, 11971, 15576,\n",
            "         3809, 14059,  2167,  7718,  8996, 14869, 16404,  8085, 16404,  8996,\n",
            "         5817,  3262,  5793, 10350, 12576, 13551, 12857,  7047, 15311,  8996,\n",
            "        11622,   443,  2773,  5754, 10875,  5110, 15576,  8996,  5817, 13810,\n",
            "        12534,  6003,  8996,   252, 12750,  8996, 11622,   443,   808,  1662,\n",
            "         4281,  3209, 12576,  3554,  8996,  5817, 13810, 12534,  5549,  8996,\n",
            "          443,  1163,  7548,  3759,  4784,  2167,  6201, 10900,  4956,  4872,\n",
            "        14137, 16404,  8996,  5817,  3262,  5793,  8996, 14374,  6290, 16404,\n",
            "        15119, 13752,  8154,  3759,  7071,  5215, 16404, 12963, 16404, 12534,\n",
            "        11031,  3838, 12443,  4281,  8996,   443, 16404, 12632,  7047,  3809,\n",
            "        16104, 12534,  3911,  4281,  7713,  5528, 10408,   443,  2920,  8996,\n",
            "        14374,  6290, 16285,  4281,  3684,  5881,  5274,  7047, 16323,  1149,\n",
            "         5572,  2167,  6363,  5817,  1758,  8996, 14374,  6290, 16404, 13128,\n",
            "         3544,  7422,   757, 11420, 11582,  2581,  4281,  2253,   808,  3262,\n",
            "        14970, 11413, 12750,  8996, 11486, 13550, 16404,  1149,  3935, 12750,\n",
            "        14019,  8154, 12534, 11754, 13752,  3759,  8996, 11802,   443,  2167,\n",
            "         4872,  1957,  3259,  7047, 11835,  1149, 15603,  8751,  4281,  8996,\n",
            "          443, 16404, 15119, 15723,  1033, 10754,  4281,  8996, 12402,  2722,\n",
            "        12750,  5500,  5806, 16404,  8505,  1249,  2246,  5934,  4844,  7047,\n",
            "         6348, 16404,  3911,  4281,  8996,  6348,  1021,  3759,  5074,  5689,\n",
            "        16404,   377,  2167,  6363,  5817, 13179, 11551,  1758, 11031, 12750,\n",
            "         8996, 11486,  1608,  4281,   377, 16404,   757,  4784, 14378,  8849,\n",
            "        14448,  3759,  1410,  4281, 10759,  7047,  2825,  7529,  3262, 14970,\n",
            "         1786,  7047, 14448,  8996,   443,  2167,  6363, 11802,   443, 12366,\n",
            "          933, 16404,  7587,   428,  8996,  4151,  4281,  8059,  4815,  4281,\n",
            "         2317, 12534, 11551, 15110,  8996,  6348, 10805, 12409,  4281,  6348,\n",
            "        12259,  3759,  3001,  6889, 16404, 16218, 16404, 15119, 11294,  3612,\n",
            "         3544,  2167,  6363,  5817, 12366, 15576, 14750,  6348, 12259, 12534,\n",
            "        16479,  1149,  4386, 12750,  7842,  1513, 16404, 15119,  3523,  1465,\n",
            "         5500,  1163, 10179,  1254,  2167,  4872,  8361, 13178, 16404,  4281,\n",
            "        13859, 12750,  5500, 16404, 10609, 12750,  8996, 11802,   443,  1005,\n",
            "        15318,  7047,  7274, 16404,  9400, 11420,  6625,  8996,  4796, 16626,\n",
            "        16285, 12534, 14698, 15121, 11165,  7047,  1662, 14970,  3809, 10858,\n",
            "         2167, 10424, 12750,  1662, 13551,  4225, 13182,  1737, 11035,  5166,\n",
            "         9343,  1737, 14995,  5166, 16404, 12534,  4909,  8266, 14378, 13702,\n",
            "         7047,  8996,  2089, 13551,  4225, 13182,  1737, 16612,  5166,  9343,\n",
            "         1737,   179,  2167,  5166,  4872, 10763,  8786, 16404,  8996,  5817,\n",
            "        13810, 11138,  5500,  7047,  8501,  4281,  1149, 12353, 12750, 10327,\n",
            "        10781, 12534,  8751, 16404, 12534,  4784,  1005,  1957, 10759,  7047,\n",
            "         6804, 11622,   505,   749,  2167, 11622,  6567, 12189,  8114, 16404,\n",
            "         1149, 29998])\n",
            "Input: tensor([11622,  9706, 15723,  1957,   426, 12534,  7721, 11662, 12576,  2818,\n",
            "         4281,  5817,  8019,   593, 14137, 12534, 10173,  2167,  4872,  8996,\n",
            "         9522, 11622,  9900,  9655, 16404, 15119,  9779,  5528,  8786,  7047,\n",
            "        10173,  4644,  8996,  6870, 15602, 10544, 12750,  8996, 11622,  9706,\n",
            "        16404,  8996,   488,  6266, 15046,  7379, 13757,  5528,  8996,  5817,\n",
            "         2089, 12534,  5744,  8996,  7458,  5413, 13182,  8996,  7587, 10444,\n",
            "         1154, 12353,  8504,  3759,  1578, 13447, 12750,  2307, 12534,  2732,\n",
            "         2112,  2167, 11622,  4909, 13782,  7047, 13400,  5316, 15576,  8996,\n",
            "         5817,  3262, 16404,  1149,   115,  4281, 15119, 11420, 11221,  1938,\n",
            "        10875,  5320,  2167, 13162,  7047,  8996, 14869, 16404,  5817,  3809,\n",
            "         4792,  4950,  8996,   443,  1149, 13755,  3289, 11717, 12750,  1254,\n",
            "         4281,  7379,  1477, 11655, 16404, 15119, 13551, 14436, 11971, 15576,\n",
            "         3809, 14059,  2167,  7718,  8996, 14869, 16404,  8085, 16404,  8996,\n",
            "         5817,  3262,  5793, 10350, 12576, 13551, 12857,  7047, 15311,  8996,\n",
            "        11622,   443,  2773,  5754, 10875,  5110, 15576,  8996,  5817, 13810,\n",
            "        12534,  6003,  8996,   252, 12750,  8996, 11622,   443,   808,  1662,\n",
            "         4281,  3209, 12576,  3554,  8996,  5817, 13810, 12534,  5549,  8996,\n",
            "          443,  1163,  7548,  3759,  4784,  2167,  6201, 10900,  4956,  4872,\n",
            "        14137, 16404,  8996,  5817,  3262,  5793,  8996, 14374,  6290, 16404,\n",
            "        15119, 13752,  8154,  3759,  7071,  5215, 16404, 12963, 16404, 12534,\n",
            "        11031,  3838, 12443,  4281,  8996,   443, 16404, 12632,  7047,  3809,\n",
            "        16104, 12534,  3911,  4281,  7713,  5528, 10408,   443,  2920,  8996,\n",
            "        14374,  6290, 16285,  4281,  3684,  5881,  5274,  7047, 16323,  1149,\n",
            "         5572,  2167,  6363,  5817,  1758,  8996, 14374,  6290, 16404, 13128,\n",
            "         3544,  7422,   757, 11420, 11582,  2581,  4281,  2253,   808,  3262,\n",
            "        14970, 11413, 12750,  8996, 11486, 13550, 16404,  1149,  3935, 12750,\n",
            "        14019,  8154, 12534, 11754, 13752,  3759,  8996, 11802,   443,  2167,\n",
            "         4872,  1957,  3259,  7047, 11835,  1149, 15603,  8751,  4281,  8996,\n",
            "          443, 16404, 15119, 15723,  1033, 10754,  4281,  8996, 12402,  2722,\n",
            "        12750,  5500,  5806, 16404,  8505,  1249,  2246,  5934,  4844,  7047,\n",
            "         6348, 16404,  3911,  4281,  8996,  6348,  1021,  3759,  5074,  5689,\n",
            "        16404,   377,  2167,  6363,  5817, 13179, 11551,  1758, 11031, 12750,\n",
            "         8996, 11486,  1608,  4281,   377, 16404,   757,  4784, 14378,  8849,\n",
            "        14448,  3759,  1410,  4281, 10759,  7047,  2825,  7529,  3262, 14970,\n",
            "         1786,  7047, 14448,  8996,   443,  2167,  6363, 11802,   443, 12366,\n",
            "          933, 16404,  7587,   428,  8996,  4151,  4281,  8059,  4815,  4281,\n",
            "         2317, 12534, 11551, 15110,  8996,  6348, 10805, 12409,  4281,  6348,\n",
            "        12259,  3759,  3001,  6889, 16404, 16218, 16404, 15119, 11294,  3612,\n",
            "         3544,  2167,  6363,  5817, 12366, 15576, 14750,  6348, 12259, 12534,\n",
            "        16479,  1149,  4386, 12750,  7842,  1513, 16404, 15119,  3523,  1465,\n",
            "         5500,  1163, 10179,  1254,  2167,  4872,  8361, 13178, 16404,  4281,\n",
            "        13859, 12750,  5500, 16404, 10609, 12750,  8996, 11802,   443,  1005,\n",
            "        15318,  7047,  7274, 16404,  9400, 11420,  6625,  8996,  4796, 16626,\n",
            "        16285, 12534, 14698, 15121, 11165,  7047,  1662, 14970,  3809, 10858,\n",
            "         2167, 10424, 12750,  1662, 13551,  4225, 13182,  1737, 11035,  5166,\n",
            "         9343,  1737, 14995,  5166, 16404, 12534,  4909,  8266, 14378, 13702,\n",
            "         7047,  8996,  2089, 13551,  4225, 13182,  1737, 16612,  5166,  9343,\n",
            "         1737,   179,  2167,  5166,  4872, 10763,  8786, 16404,  8996,  5817,\n",
            "        13810, 11138,  5500,  7047,  8501,  4281,  1149, 12353, 12750, 10327,\n",
            "        10781, 12534,  8751, 16404, 12534,  4784,  1005,  1957, 10759,  7047,\n",
            "         6804, 11622,   505,   749,  2167, 11622,  6567, 12189,  8114, 16404,\n",
            "         1149,  5198])\n",
            "Target: tensor([ 9706, 15723,  1957,   426, 12534,  7721, 11662, 12576,  2818,  4281,\n",
            "         5817,  8019,   593, 14137, 12534, 10173,  2167,  4872,  8996,  9522,\n",
            "        11622,  9900,  9655, 16404, 15119,  9779,  5528,  8786,  7047, 10173,\n",
            "         4644,  8996,  6870, 15602, 10544, 12750,  8996, 11622,  9706, 16404,\n",
            "         8996,   488,  6266, 15046,  7379, 13757,  5528,  8996,  5817,  2089,\n",
            "        12534,  5744,  8996,  7458,  5413, 13182,  8996,  7587, 10444,  1154,\n",
            "        12353,  8504,  3759,  1578, 13447, 12750,  2307, 12534,  2732,  2112,\n",
            "         2167, 11622,  4909, 13782,  7047, 13400,  5316, 15576,  8996,  5817,\n",
            "         3262, 16404,  1149,   115,  4281, 15119, 11420, 11221,  1938, 10875,\n",
            "         5320,  2167, 13162,  7047,  8996, 14869, 16404,  5817,  3809,  4792,\n",
            "         4950,  8996,   443,  1149, 13755,  3289, 11717, 12750,  1254,  4281,\n",
            "         7379,  1477, 11655, 16404, 15119, 13551, 14436, 11971, 15576,  3809,\n",
            "        14059,  2167,  7718,  8996, 14869, 16404,  8085, 16404,  8996,  5817,\n",
            "         3262,  5793, 10350, 12576, 13551, 12857,  7047, 15311,  8996, 11622,\n",
            "          443,  2773,  5754, 10875,  5110, 15576,  8996,  5817, 13810, 12534,\n",
            "         6003,  8996,   252, 12750,  8996, 11622,   443,   808,  1662,  4281,\n",
            "         3209, 12576,  3554,  8996,  5817, 13810, 12534,  5549,  8996,   443,\n",
            "         1163,  7548,  3759,  4784,  2167,  6201, 10900,  4956,  4872, 14137,\n",
            "        16404,  8996,  5817,  3262,  5793,  8996, 14374,  6290, 16404, 15119,\n",
            "        13752,  8154,  3759,  7071,  5215, 16404, 12963, 16404, 12534, 11031,\n",
            "         3838, 12443,  4281,  8996,   443, 16404, 12632,  7047,  3809, 16104,\n",
            "        12534,  3911,  4281,  7713,  5528, 10408,   443,  2920,  8996, 14374,\n",
            "         6290, 16285,  4281,  3684,  5881,  5274,  7047, 16323,  1149,  5572,\n",
            "         2167,  6363,  5817,  1758,  8996, 14374,  6290, 16404, 13128,  3544,\n",
            "         7422,   757, 11420, 11582,  2581,  4281,  2253,   808,  3262, 14970,\n",
            "        11413, 12750,  8996, 11486, 13550, 16404,  1149,  3935, 12750, 14019,\n",
            "         8154, 12534, 11754, 13752,  3759,  8996, 11802,   443,  2167,  4872,\n",
            "         1957,  3259,  7047, 11835,  1149, 15603,  8751,  4281,  8996,   443,\n",
            "        16404, 15119, 15723,  1033, 10754,  4281,  8996, 12402,  2722, 12750,\n",
            "         5500,  5806, 16404,  8505,  1249,  2246,  5934,  4844,  7047,  6348,\n",
            "        16404,  3911,  4281,  8996,  6348,  1021,  3759,  5074,  5689, 16404,\n",
            "          377,  2167,  6363,  5817, 13179, 11551,  1758, 11031, 12750,  8996,\n",
            "        11486,  1608,  4281,   377, 16404,   757,  4784, 14378,  8849, 14448,\n",
            "         3759,  1410,  4281, 10759,  7047,  2825,  7529,  3262, 14970,  1786,\n",
            "         7047, 14448,  8996,   443,  2167,  6363, 11802,   443, 12366,   933,\n",
            "        16404,  7587,   428,  8996,  4151,  4281,  8059,  4815,  4281,  2317,\n",
            "        12534, 11551, 15110,  8996,  6348, 10805, 12409,  4281,  6348, 12259,\n",
            "         3759,  3001,  6889, 16404, 16218, 16404, 15119, 11294,  3612,  3544,\n",
            "         2167,  6363,  5817, 12366, 15576, 14750,  6348, 12259, 12534, 16479,\n",
            "         1149,  4386, 12750,  7842,  1513, 16404, 15119,  3523,  1465,  5500,\n",
            "         1163, 10179,  1254,  2167,  4872,  8361, 13178, 16404,  4281, 13859,\n",
            "        12750,  5500, 16404, 10609, 12750,  8996, 11802,   443,  1005, 15318,\n",
            "         7047,  7274, 16404,  9400, 11420,  6625,  8996,  4796, 16626, 16285,\n",
            "        12534, 14698, 15121, 11165,  7047,  1662, 14970,  3809, 10858,  2167,\n",
            "        10424, 12750,  1662, 13551,  4225, 13182,  1737, 11035,  5166,  9343,\n",
            "         1737, 14995,  5166, 16404, 12534,  4909,  8266, 14378, 13702,  7047,\n",
            "         8996,  2089, 13551,  4225, 13182,  1737, 16612,  5166,  9343,  1737,\n",
            "          179,  2167,  5166,  4872, 10763,  8786, 16404,  8996,  5817, 13810,\n",
            "        11138,  5500,  7047,  8501,  4281,  1149, 12353, 12750, 10327, 10781,\n",
            "        12534,  8751, 16404, 12534,  4784,  1005,  1957, 10759,  7047,  6804,\n",
            "        11622,   505,   749,  2167, 11622,  6567, 12189,  8114, 16404,  1149,\n",
            "         5198, 29998])\n",
            "Input: tensor([ 9706, 15723,  1957,   426, 12534,  7721, 11662, 12576,  2818,  4281,\n",
            "         5817,  8019,   593, 14137, 12534, 10173,  2167,  4872,  8996,  9522,\n",
            "        11622,  9900,  9655, 16404, 15119,  9779,  5528,  8786,  7047, 10173,\n",
            "         4644,  8996,  6870, 15602, 10544, 12750,  8996, 11622,  9706, 16404,\n",
            "         8996,   488,  6266, 15046,  7379, 13757,  5528,  8996,  5817,  2089,\n",
            "        12534,  5744,  8996,  7458,  5413, 13182,  8996,  7587, 10444,  1154,\n",
            "        12353,  8504,  3759,  1578, 13447, 12750,  2307, 12534,  2732,  2112,\n",
            "         2167, 11622,  4909, 13782,  7047, 13400,  5316, 15576,  8996,  5817,\n",
            "         3262, 16404,  1149,   115,  4281, 15119, 11420, 11221,  1938, 10875,\n",
            "         5320,  2167, 13162,  7047,  8996, 14869, 16404,  5817,  3809,  4792,\n",
            "         4950,  8996,   443,  1149, 13755,  3289, 11717, 12750,  1254,  4281,\n",
            "         7379,  1477, 11655, 16404, 15119, 13551, 14436, 11971, 15576,  3809,\n",
            "        14059,  2167,  7718,  8996, 14869, 16404,  8085, 16404,  8996,  5817,\n",
            "         3262,  5793, 10350, 12576, 13551, 12857,  7047, 15311,  8996, 11622,\n",
            "          443,  2773,  5754, 10875,  5110, 15576,  8996,  5817, 13810, 12534,\n",
            "         6003,  8996,   252, 12750,  8996, 11622,   443,   808,  1662,  4281,\n",
            "         3209, 12576,  3554,  8996,  5817, 13810, 12534,  5549,  8996,   443,\n",
            "         1163,  7548,  3759,  4784,  2167,  6201, 10900,  4956,  4872, 14137,\n",
            "        16404,  8996,  5817,  3262,  5793,  8996, 14374,  6290, 16404, 15119,\n",
            "        13752,  8154,  3759,  7071,  5215, 16404, 12963, 16404, 12534, 11031,\n",
            "         3838, 12443,  4281,  8996,   443, 16404, 12632,  7047,  3809, 16104,\n",
            "        12534,  3911,  4281,  7713,  5528, 10408,   443,  2920,  8996, 14374,\n",
            "         6290, 16285,  4281,  3684,  5881,  5274,  7047, 16323,  1149,  5572,\n",
            "         2167,  6363,  5817,  1758,  8996, 14374,  6290, 16404, 13128,  3544,\n",
            "         7422,   757, 11420, 11582,  2581,  4281,  2253,   808,  3262, 14970,\n",
            "        11413, 12750,  8996, 11486, 13550, 16404,  1149,  3935, 12750, 14019,\n",
            "         8154, 12534, 11754, 13752,  3759,  8996, 11802,   443,  2167,  4872,\n",
            "         1957,  3259,  7047, 11835,  1149, 15603,  8751,  4281,  8996,   443,\n",
            "        16404, 15119, 15723,  1033, 10754,  4281,  8996, 12402,  2722, 12750,\n",
            "         5500,  5806, 16404,  8505,  1249,  2246,  5934,  4844,  7047,  6348,\n",
            "        16404,  3911,  4281,  8996,  6348,  1021,  3759,  5074,  5689, 16404,\n",
            "          377,  2167,  6363,  5817, 13179, 11551,  1758, 11031, 12750,  8996,\n",
            "        11486,  1608,  4281,   377, 16404,   757,  4784, 14378,  8849, 14448,\n",
            "         3759,  1410,  4281, 10759,  7047,  2825,  7529,  3262, 14970,  1786,\n",
            "         7047, 14448,  8996,   443,  2167,  6363, 11802,   443, 12366,   933,\n",
            "        16404,  7587,   428,  8996,  4151,  4281,  8059,  4815,  4281,  2317,\n",
            "        12534, 11551, 15110,  8996,  6348, 10805, 12409,  4281,  6348, 12259,\n",
            "         3759,  3001,  6889, 16404, 16218, 16404, 15119, 11294,  3612,  3544,\n",
            "         2167,  6363,  5817, 12366, 15576, 14750,  6348, 12259, 12534, 16479,\n",
            "         1149,  4386, 12750,  7842,  1513, 16404, 15119,  3523,  1465,  5500,\n",
            "         1163, 10179,  1254,  2167,  4872,  8361, 13178, 16404,  4281, 13859,\n",
            "        12750,  5500, 16404, 10609, 12750,  8996, 11802,   443,  1005, 15318,\n",
            "         7047,  7274, 16404,  9400, 11420,  6625,  8996,  4796, 16626, 16285,\n",
            "        12534, 14698, 15121, 11165,  7047,  1662, 14970,  3809, 10858,  2167,\n",
            "        10424, 12750,  1662, 13551,  4225, 13182,  1737, 11035,  5166,  9343,\n",
            "         1737, 14995,  5166, 16404, 12534,  4909,  8266, 14378, 13702,  7047,\n",
            "         8996,  2089, 13551,  4225, 13182,  1737, 16612,  5166,  9343,  1737,\n",
            "          179,  2167,  5166,  4872, 10763,  8786, 16404,  8996,  5817, 13810,\n",
            "        11138,  5500,  7047,  8501,  4281,  1149, 12353, 12750, 10327, 10781,\n",
            "        12534,  8751, 16404, 12534,  4784,  1005,  1957, 10759,  7047,  6804,\n",
            "        11622,   505,   749,  2167, 11622,  6567, 12189,  8114, 16404,  1149,\n",
            "         5198, 12750])\n",
            "Target: tensor([15723,  1957,   426, 12534,  7721, 11662, 12576,  2818,  4281,  5817,\n",
            "         8019,   593, 14137, 12534, 10173,  2167,  4872,  8996,  9522, 11622,\n",
            "         9900,  9655, 16404, 15119,  9779,  5528,  8786,  7047, 10173,  4644,\n",
            "         8996,  6870, 15602, 10544, 12750,  8996, 11622,  9706, 16404,  8996,\n",
            "          488,  6266, 15046,  7379, 13757,  5528,  8996,  5817,  2089, 12534,\n",
            "         5744,  8996,  7458,  5413, 13182,  8996,  7587, 10444,  1154, 12353,\n",
            "         8504,  3759,  1578, 13447, 12750,  2307, 12534,  2732,  2112,  2167,\n",
            "        11622,  4909, 13782,  7047, 13400,  5316, 15576,  8996,  5817,  3262,\n",
            "        16404,  1149,   115,  4281, 15119, 11420, 11221,  1938, 10875,  5320,\n",
            "         2167, 13162,  7047,  8996, 14869, 16404,  5817,  3809,  4792,  4950,\n",
            "         8996,   443,  1149, 13755,  3289, 11717, 12750,  1254,  4281,  7379,\n",
            "         1477, 11655, 16404, 15119, 13551, 14436, 11971, 15576,  3809, 14059,\n",
            "         2167,  7718,  8996, 14869, 16404,  8085, 16404,  8996,  5817,  3262,\n",
            "         5793, 10350, 12576, 13551, 12857,  7047, 15311,  8996, 11622,   443,\n",
            "         2773,  5754, 10875,  5110, 15576,  8996,  5817, 13810, 12534,  6003,\n",
            "         8996,   252, 12750,  8996, 11622,   443,   808,  1662,  4281,  3209,\n",
            "        12576,  3554,  8996,  5817, 13810, 12534,  5549,  8996,   443,  1163,\n",
            "         7548,  3759,  4784,  2167,  6201, 10900,  4956,  4872, 14137, 16404,\n",
            "         8996,  5817,  3262,  5793,  8996, 14374,  6290, 16404, 15119, 13752,\n",
            "         8154,  3759,  7071,  5215, 16404, 12963, 16404, 12534, 11031,  3838,\n",
            "        12443,  4281,  8996,   443, 16404, 12632,  7047,  3809, 16104, 12534,\n",
            "         3911,  4281,  7713,  5528, 10408,   443,  2920,  8996, 14374,  6290,\n",
            "        16285,  4281,  3684,  5881,  5274,  7047, 16323,  1149,  5572,  2167,\n",
            "         6363,  5817,  1758,  8996, 14374,  6290, 16404, 13128,  3544,  7422,\n",
            "          757, 11420, 11582,  2581,  4281,  2253,   808,  3262, 14970, 11413,\n",
            "        12750,  8996, 11486, 13550, 16404,  1149,  3935, 12750, 14019,  8154,\n",
            "        12534, 11754, 13752,  3759,  8996, 11802,   443,  2167,  4872,  1957,\n",
            "         3259,  7047, 11835,  1149, 15603,  8751,  4281,  8996,   443, 16404,\n",
            "        15119, 15723,  1033, 10754,  4281,  8996, 12402,  2722, 12750,  5500,\n",
            "         5806, 16404,  8505,  1249,  2246,  5934,  4844,  7047,  6348, 16404,\n",
            "         3911,  4281,  8996,  6348,  1021,  3759,  5074,  5689, 16404,   377,\n",
            "         2167,  6363,  5817, 13179, 11551,  1758, 11031, 12750,  8996, 11486,\n",
            "         1608,  4281,   377, 16404,   757,  4784, 14378,  8849, 14448,  3759,\n",
            "         1410,  4281, 10759,  7047,  2825,  7529,  3262, 14970,  1786,  7047,\n",
            "        14448,  8996,   443,  2167,  6363, 11802,   443, 12366,   933, 16404,\n",
            "         7587,   428,  8996,  4151,  4281,  8059,  4815,  4281,  2317, 12534,\n",
            "        11551, 15110,  8996,  6348, 10805, 12409,  4281,  6348, 12259,  3759,\n",
            "         3001,  6889, 16404, 16218, 16404, 15119, 11294,  3612,  3544,  2167,\n",
            "         6363,  5817, 12366, 15576, 14750,  6348, 12259, 12534, 16479,  1149,\n",
            "         4386, 12750,  7842,  1513, 16404, 15119,  3523,  1465,  5500,  1163,\n",
            "        10179,  1254,  2167,  4872,  8361, 13178, 16404,  4281, 13859, 12750,\n",
            "         5500, 16404, 10609, 12750,  8996, 11802,   443,  1005, 15318,  7047,\n",
            "         7274, 16404,  9400, 11420,  6625,  8996,  4796, 16626, 16285, 12534,\n",
            "        14698, 15121, 11165,  7047,  1662, 14970,  3809, 10858,  2167, 10424,\n",
            "        12750,  1662, 13551,  4225, 13182,  1737, 11035,  5166,  9343,  1737,\n",
            "        14995,  5166, 16404, 12534,  4909,  8266, 14378, 13702,  7047,  8996,\n",
            "         2089, 13551,  4225, 13182,  1737, 16612,  5166,  9343,  1737,   179,\n",
            "         2167,  5166,  4872, 10763,  8786, 16404,  8996,  5817, 13810, 11138,\n",
            "         5500,  7047,  8501,  4281,  1149, 12353, 12750, 10327, 10781, 12534,\n",
            "         8751, 16404, 12534,  4784,  1005,  1957, 10759,  7047,  6804, 11622,\n",
            "          505,   749,  2167, 11622,  6567, 12189,  8114, 16404,  1149,  5198,\n",
            "        12750, 29998])\n",
            "Input: tensor([15723,  1957,   426, 12534,  7721, 11662, 12576,  2818,  4281,  5817,\n",
            "         8019,   593, 14137, 12534, 10173,  2167,  4872,  8996,  9522, 11622,\n",
            "         9900,  9655, 16404, 15119,  9779,  5528,  8786,  7047, 10173,  4644,\n",
            "         8996,  6870, 15602, 10544, 12750,  8996, 11622,  9706, 16404,  8996,\n",
            "          488,  6266, 15046,  7379, 13757,  5528,  8996,  5817,  2089, 12534,\n",
            "         5744,  8996,  7458,  5413, 13182,  8996,  7587, 10444,  1154, 12353,\n",
            "         8504,  3759,  1578, 13447, 12750,  2307, 12534,  2732,  2112,  2167,\n",
            "        11622,  4909, 13782,  7047, 13400,  5316, 15576,  8996,  5817,  3262,\n",
            "        16404,  1149,   115,  4281, 15119, 11420, 11221,  1938, 10875,  5320,\n",
            "         2167, 13162,  7047,  8996, 14869, 16404,  5817,  3809,  4792,  4950,\n",
            "         8996,   443,  1149, 13755,  3289, 11717, 12750,  1254,  4281,  7379,\n",
            "         1477, 11655, 16404, 15119, 13551, 14436, 11971, 15576,  3809, 14059,\n",
            "         2167,  7718,  8996, 14869, 16404,  8085, 16404,  8996,  5817,  3262,\n",
            "         5793, 10350, 12576, 13551, 12857,  7047, 15311,  8996, 11622,   443,\n",
            "         2773,  5754, 10875,  5110, 15576,  8996,  5817, 13810, 12534,  6003,\n",
            "         8996,   252, 12750,  8996, 11622,   443,   808,  1662,  4281,  3209,\n",
            "        12576,  3554,  8996,  5817, 13810, 12534,  5549,  8996,   443,  1163,\n",
            "         7548,  3759,  4784,  2167,  6201, 10900,  4956,  4872, 14137, 16404,\n",
            "         8996,  5817,  3262,  5793,  8996, 14374,  6290, 16404, 15119, 13752,\n",
            "         8154,  3759,  7071,  5215, 16404, 12963, 16404, 12534, 11031,  3838,\n",
            "        12443,  4281,  8996,   443, 16404, 12632,  7047,  3809, 16104, 12534,\n",
            "         3911,  4281,  7713,  5528, 10408,   443,  2920,  8996, 14374,  6290,\n",
            "        16285,  4281,  3684,  5881,  5274,  7047, 16323,  1149,  5572,  2167,\n",
            "         6363,  5817,  1758,  8996, 14374,  6290, 16404, 13128,  3544,  7422,\n",
            "          757, 11420, 11582,  2581,  4281,  2253,   808,  3262, 14970, 11413,\n",
            "        12750,  8996, 11486, 13550, 16404,  1149,  3935, 12750, 14019,  8154,\n",
            "        12534, 11754, 13752,  3759,  8996, 11802,   443,  2167,  4872,  1957,\n",
            "         3259,  7047, 11835,  1149, 15603,  8751,  4281,  8996,   443, 16404,\n",
            "        15119, 15723,  1033, 10754,  4281,  8996, 12402,  2722, 12750,  5500,\n",
            "         5806, 16404,  8505,  1249,  2246,  5934,  4844,  7047,  6348, 16404,\n",
            "         3911,  4281,  8996,  6348,  1021,  3759,  5074,  5689, 16404,   377,\n",
            "         2167,  6363,  5817, 13179, 11551,  1758, 11031, 12750,  8996, 11486,\n",
            "         1608,  4281,   377, 16404,   757,  4784, 14378,  8849, 14448,  3759,\n",
            "         1410,  4281, 10759,  7047,  2825,  7529,  3262, 14970,  1786,  7047,\n",
            "        14448,  8996,   443,  2167,  6363, 11802,   443, 12366,   933, 16404,\n",
            "         7587,   428,  8996,  4151,  4281,  8059,  4815,  4281,  2317, 12534,\n",
            "        11551, 15110,  8996,  6348, 10805, 12409,  4281,  6348, 12259,  3759,\n",
            "         3001,  6889, 16404, 16218, 16404, 15119, 11294,  3612,  3544,  2167,\n",
            "         6363,  5817, 12366, 15576, 14750,  6348, 12259, 12534, 16479,  1149,\n",
            "         4386, 12750,  7842,  1513, 16404, 15119,  3523,  1465,  5500,  1163,\n",
            "        10179,  1254,  2167,  4872,  8361, 13178, 16404,  4281, 13859, 12750,\n",
            "         5500, 16404, 10609, 12750,  8996, 11802,   443,  1005, 15318,  7047,\n",
            "         7274, 16404,  9400, 11420,  6625,  8996,  4796, 16626, 16285, 12534,\n",
            "        14698, 15121, 11165,  7047,  1662, 14970,  3809, 10858,  2167, 10424,\n",
            "        12750,  1662, 13551,  4225, 13182,  1737, 11035,  5166,  9343,  1737,\n",
            "        14995,  5166, 16404, 12534,  4909,  8266, 14378, 13702,  7047,  8996,\n",
            "         2089, 13551,  4225, 13182,  1737, 16612,  5166,  9343,  1737,   179,\n",
            "         2167,  5166,  4872, 10763,  8786, 16404,  8996,  5817, 13810, 11138,\n",
            "         5500,  7047,  8501,  4281,  1149, 12353, 12750, 10327, 10781, 12534,\n",
            "         8751, 16404, 12534,  4784,  1005,  1957, 10759,  7047,  6804, 11622,\n",
            "          505,   749,  2167, 11622,  6567, 12189,  8114, 16404,  1149,  5198,\n",
            "        12750,  8996])\n",
            "Target: tensor([ 1957,   426, 12534,  7721, 11662, 12576,  2818,  4281,  5817,  8019,\n",
            "          593, 14137, 12534, 10173,  2167,  4872,  8996,  9522, 11622,  9900,\n",
            "         9655, 16404, 15119,  9779,  5528,  8786,  7047, 10173,  4644,  8996,\n",
            "         6870, 15602, 10544, 12750,  8996, 11622,  9706, 16404,  8996,   488,\n",
            "         6266, 15046,  7379, 13757,  5528,  8996,  5817,  2089, 12534,  5744,\n",
            "         8996,  7458,  5413, 13182,  8996,  7587, 10444,  1154, 12353,  8504,\n",
            "         3759,  1578, 13447, 12750,  2307, 12534,  2732,  2112,  2167, 11622,\n",
            "         4909, 13782,  7047, 13400,  5316, 15576,  8996,  5817,  3262, 16404,\n",
            "         1149,   115,  4281, 15119, 11420, 11221,  1938, 10875,  5320,  2167,\n",
            "        13162,  7047,  8996, 14869, 16404,  5817,  3809,  4792,  4950,  8996,\n",
            "          443,  1149, 13755,  3289, 11717, 12750,  1254,  4281,  7379,  1477,\n",
            "        11655, 16404, 15119, 13551, 14436, 11971, 15576,  3809, 14059,  2167,\n",
            "         7718,  8996, 14869, 16404,  8085, 16404,  8996,  5817,  3262,  5793,\n",
            "        10350, 12576, 13551, 12857,  7047, 15311,  8996, 11622,   443,  2773,\n",
            "         5754, 10875,  5110, 15576,  8996,  5817, 13810, 12534,  6003,  8996,\n",
            "          252, 12750,  8996, 11622,   443,   808,  1662,  4281,  3209, 12576,\n",
            "         3554,  8996,  5817, 13810, 12534,  5549,  8996,   443,  1163,  7548,\n",
            "         3759,  4784,  2167,  6201, 10900,  4956,  4872, 14137, 16404,  8996,\n",
            "         5817,  3262,  5793,  8996, 14374,  6290, 16404, 15119, 13752,  8154,\n",
            "         3759,  7071,  5215, 16404, 12963, 16404, 12534, 11031,  3838, 12443,\n",
            "         4281,  8996,   443, 16404, 12632,  7047,  3809, 16104, 12534,  3911,\n",
            "         4281,  7713,  5528, 10408,   443,  2920,  8996, 14374,  6290, 16285,\n",
            "         4281,  3684,  5881,  5274,  7047, 16323,  1149,  5572,  2167,  6363,\n",
            "         5817,  1758,  8996, 14374,  6290, 16404, 13128,  3544,  7422,   757,\n",
            "        11420, 11582,  2581,  4281,  2253,   808,  3262, 14970, 11413, 12750,\n",
            "         8996, 11486, 13550, 16404,  1149,  3935, 12750, 14019,  8154, 12534,\n",
            "        11754, 13752,  3759,  8996, 11802,   443,  2167,  4872,  1957,  3259,\n",
            "         7047, 11835,  1149, 15603,  8751,  4281,  8996,   443, 16404, 15119,\n",
            "        15723,  1033, 10754,  4281,  8996, 12402,  2722, 12750,  5500,  5806,\n",
            "        16404,  8505,  1249,  2246,  5934,  4844,  7047,  6348, 16404,  3911,\n",
            "         4281,  8996,  6348,  1021,  3759,  5074,  5689, 16404,   377,  2167,\n",
            "         6363,  5817, 13179, 11551,  1758, 11031, 12750,  8996, 11486,  1608,\n",
            "         4281,   377, 16404,   757,  4784, 14378,  8849, 14448,  3759,  1410,\n",
            "         4281, 10759,  7047,  2825,  7529,  3262, 14970,  1786,  7047, 14448,\n",
            "         8996,   443,  2167,  6363, 11802,   443, 12366,   933, 16404,  7587,\n",
            "          428,  8996,  4151,  4281,  8059,  4815,  4281,  2317, 12534, 11551,\n",
            "        15110,  8996,  6348, 10805, 12409,  4281,  6348, 12259,  3759,  3001,\n",
            "         6889, 16404, 16218, 16404, 15119, 11294,  3612,  3544,  2167,  6363,\n",
            "         5817, 12366, 15576, 14750,  6348, 12259, 12534, 16479,  1149,  4386,\n",
            "        12750,  7842,  1513, 16404, 15119,  3523,  1465,  5500,  1163, 10179,\n",
            "         1254,  2167,  4872,  8361, 13178, 16404,  4281, 13859, 12750,  5500,\n",
            "        16404, 10609, 12750,  8996, 11802,   443,  1005, 15318,  7047,  7274,\n",
            "        16404,  9400, 11420,  6625,  8996,  4796, 16626, 16285, 12534, 14698,\n",
            "        15121, 11165,  7047,  1662, 14970,  3809, 10858,  2167, 10424, 12750,\n",
            "         1662, 13551,  4225, 13182,  1737, 11035,  5166,  9343,  1737, 14995,\n",
            "         5166, 16404, 12534,  4909,  8266, 14378, 13702,  7047,  8996,  2089,\n",
            "        13551,  4225, 13182,  1737, 16612,  5166,  9343,  1737,   179,  2167,\n",
            "         5166,  4872, 10763,  8786, 16404,  8996,  5817, 13810, 11138,  5500,\n",
            "         7047,  8501,  4281,  1149, 12353, 12750, 10327, 10781, 12534,  8751,\n",
            "        16404, 12534,  4784,  1005,  1957, 10759,  7047,  6804, 11622,   505,\n",
            "          749,  2167, 11622,  6567, 12189,  8114, 16404,  1149,  5198, 12750,\n",
            "         8996, 29998])\n",
            "Input: tensor([ 1957,   426, 12534,  7721, 11662, 12576,  2818,  4281,  5817,  8019,\n",
            "          593, 14137, 12534, 10173,  2167,  4872,  8996,  9522, 11622,  9900,\n",
            "         9655, 16404, 15119,  9779,  5528,  8786,  7047, 10173,  4644,  8996,\n",
            "         6870, 15602, 10544, 12750,  8996, 11622,  9706, 16404,  8996,   488,\n",
            "         6266, 15046,  7379, 13757,  5528,  8996,  5817,  2089, 12534,  5744,\n",
            "         8996,  7458,  5413, 13182,  8996,  7587, 10444,  1154, 12353,  8504,\n",
            "         3759,  1578, 13447, 12750,  2307, 12534,  2732,  2112,  2167, 11622,\n",
            "         4909, 13782,  7047, 13400,  5316, 15576,  8996,  5817,  3262, 16404,\n",
            "         1149,   115,  4281, 15119, 11420, 11221,  1938, 10875,  5320,  2167,\n",
            "        13162,  7047,  8996, 14869, 16404,  5817,  3809,  4792,  4950,  8996,\n",
            "          443,  1149, 13755,  3289, 11717, 12750,  1254,  4281,  7379,  1477,\n",
            "        11655, 16404, 15119, 13551, 14436, 11971, 15576,  3809, 14059,  2167,\n",
            "         7718,  8996, 14869, 16404,  8085, 16404,  8996,  5817,  3262,  5793,\n",
            "        10350, 12576, 13551, 12857,  7047, 15311,  8996, 11622,   443,  2773,\n",
            "         5754, 10875,  5110, 15576,  8996,  5817, 13810, 12534,  6003,  8996,\n",
            "          252, 12750,  8996, 11622,   443,   808,  1662,  4281,  3209, 12576,\n",
            "         3554,  8996,  5817, 13810, 12534,  5549,  8996,   443,  1163,  7548,\n",
            "         3759,  4784,  2167,  6201, 10900,  4956,  4872, 14137, 16404,  8996,\n",
            "         5817,  3262,  5793,  8996, 14374,  6290, 16404, 15119, 13752,  8154,\n",
            "         3759,  7071,  5215, 16404, 12963, 16404, 12534, 11031,  3838, 12443,\n",
            "         4281,  8996,   443, 16404, 12632,  7047,  3809, 16104, 12534,  3911,\n",
            "         4281,  7713,  5528, 10408,   443,  2920,  8996, 14374,  6290, 16285,\n",
            "         4281,  3684,  5881,  5274,  7047, 16323,  1149,  5572,  2167,  6363,\n",
            "         5817,  1758,  8996, 14374,  6290, 16404, 13128,  3544,  7422,   757,\n",
            "        11420, 11582,  2581,  4281,  2253,   808,  3262, 14970, 11413, 12750,\n",
            "         8996, 11486, 13550, 16404,  1149,  3935, 12750, 14019,  8154, 12534,\n",
            "        11754, 13752,  3759,  8996, 11802,   443,  2167,  4872,  1957,  3259,\n",
            "         7047, 11835,  1149, 15603,  8751,  4281,  8996,   443, 16404, 15119,\n",
            "        15723,  1033, 10754,  4281,  8996, 12402,  2722, 12750,  5500,  5806,\n",
            "        16404,  8505,  1249,  2246,  5934,  4844,  7047,  6348, 16404,  3911,\n",
            "         4281,  8996,  6348,  1021,  3759,  5074,  5689, 16404,   377,  2167,\n",
            "         6363,  5817, 13179, 11551,  1758, 11031, 12750,  8996, 11486,  1608,\n",
            "         4281,   377, 16404,   757,  4784, 14378,  8849, 14448,  3759,  1410,\n",
            "         4281, 10759,  7047,  2825,  7529,  3262, 14970,  1786,  7047, 14448,\n",
            "         8996,   443,  2167,  6363, 11802,   443, 12366,   933, 16404,  7587,\n",
            "          428,  8996,  4151,  4281,  8059,  4815,  4281,  2317, 12534, 11551,\n",
            "        15110,  8996,  6348, 10805, 12409,  4281,  6348, 12259,  3759,  3001,\n",
            "         6889, 16404, 16218, 16404, 15119, 11294,  3612,  3544,  2167,  6363,\n",
            "         5817, 12366, 15576, 14750,  6348, 12259, 12534, 16479,  1149,  4386,\n",
            "        12750,  7842,  1513, 16404, 15119,  3523,  1465,  5500,  1163, 10179,\n",
            "         1254,  2167,  4872,  8361, 13178, 16404,  4281, 13859, 12750,  5500,\n",
            "        16404, 10609, 12750,  8996, 11802,   443,  1005, 15318,  7047,  7274,\n",
            "        16404,  9400, 11420,  6625,  8996,  4796, 16626, 16285, 12534, 14698,\n",
            "        15121, 11165,  7047,  1662, 14970,  3809, 10858,  2167, 10424, 12750,\n",
            "         1662, 13551,  4225, 13182,  1737, 11035,  5166,  9343,  1737, 14995,\n",
            "         5166, 16404, 12534,  4909,  8266, 14378, 13702,  7047,  8996,  2089,\n",
            "        13551,  4225, 13182,  1737, 16612,  5166,  9343,  1737,   179,  2167,\n",
            "         5166,  4872, 10763,  8786, 16404,  8996,  5817, 13810, 11138,  5500,\n",
            "         7047,  8501,  4281,  1149, 12353, 12750, 10327, 10781, 12534,  8751,\n",
            "        16404, 12534,  4784,  1005,  1957, 10759,  7047,  6804, 11622,   505,\n",
            "          749,  2167, 11622,  6567, 12189,  8114, 16404,  1149,  5198, 12750,\n",
            "         8996, 16000])\n",
            "Target: tensor([  426, 12534,  7721, 11662, 12576,  2818,  4281,  5817,  8019,   593,\n",
            "        14137, 12534, 10173,  2167,  4872,  8996,  9522, 11622,  9900,  9655,\n",
            "        16404, 15119,  9779,  5528,  8786,  7047, 10173,  4644,  8996,  6870,\n",
            "        15602, 10544, 12750,  8996, 11622,  9706, 16404,  8996,   488,  6266,\n",
            "        15046,  7379, 13757,  5528,  8996,  5817,  2089, 12534,  5744,  8996,\n",
            "         7458,  5413, 13182,  8996,  7587, 10444,  1154, 12353,  8504,  3759,\n",
            "         1578, 13447, 12750,  2307, 12534,  2732,  2112,  2167, 11622,  4909,\n",
            "        13782,  7047, 13400,  5316, 15576,  8996,  5817,  3262, 16404,  1149,\n",
            "          115,  4281, 15119, 11420, 11221,  1938, 10875,  5320,  2167, 13162,\n",
            "         7047,  8996, 14869, 16404,  5817,  3809,  4792,  4950,  8996,   443,\n",
            "         1149, 13755,  3289, 11717, 12750,  1254,  4281,  7379,  1477, 11655,\n",
            "        16404, 15119, 13551, 14436, 11971, 15576,  3809, 14059,  2167,  7718,\n",
            "         8996, 14869, 16404,  8085, 16404,  8996,  5817,  3262,  5793, 10350,\n",
            "        12576, 13551, 12857,  7047, 15311,  8996, 11622,   443,  2773,  5754,\n",
            "        10875,  5110, 15576,  8996,  5817, 13810, 12534,  6003,  8996,   252,\n",
            "        12750,  8996, 11622,   443,   808,  1662,  4281,  3209, 12576,  3554,\n",
            "         8996,  5817, 13810, 12534,  5549,  8996,   443,  1163,  7548,  3759,\n",
            "         4784,  2167,  6201, 10900,  4956,  4872, 14137, 16404,  8996,  5817,\n",
            "         3262,  5793,  8996, 14374,  6290, 16404, 15119, 13752,  8154,  3759,\n",
            "         7071,  5215, 16404, 12963, 16404, 12534, 11031,  3838, 12443,  4281,\n",
            "         8996,   443, 16404, 12632,  7047,  3809, 16104, 12534,  3911,  4281,\n",
            "         7713,  5528, 10408,   443,  2920,  8996, 14374,  6290, 16285,  4281,\n",
            "         3684,  5881,  5274,  7047, 16323,  1149,  5572,  2167,  6363,  5817,\n",
            "         1758,  8996, 14374,  6290, 16404, 13128,  3544,  7422,   757, 11420,\n",
            "        11582,  2581,  4281,  2253,   808,  3262, 14970, 11413, 12750,  8996,\n",
            "        11486, 13550, 16404,  1149,  3935, 12750, 14019,  8154, 12534, 11754,\n",
            "        13752,  3759,  8996, 11802,   443,  2167,  4872,  1957,  3259,  7047,\n",
            "        11835,  1149, 15603,  8751,  4281,  8996,   443, 16404, 15119, 15723,\n",
            "         1033, 10754,  4281,  8996, 12402,  2722, 12750,  5500,  5806, 16404,\n",
            "         8505,  1249,  2246,  5934,  4844,  7047,  6348, 16404,  3911,  4281,\n",
            "         8996,  6348,  1021,  3759,  5074,  5689, 16404,   377,  2167,  6363,\n",
            "         5817, 13179, 11551,  1758, 11031, 12750,  8996, 11486,  1608,  4281,\n",
            "          377, 16404,   757,  4784, 14378,  8849, 14448,  3759,  1410,  4281,\n",
            "        10759,  7047,  2825,  7529,  3262, 14970,  1786,  7047, 14448,  8996,\n",
            "          443,  2167,  6363, 11802,   443, 12366,   933, 16404,  7587,   428,\n",
            "         8996,  4151,  4281,  8059,  4815,  4281,  2317, 12534, 11551, 15110,\n",
            "         8996,  6348, 10805, 12409,  4281,  6348, 12259,  3759,  3001,  6889,\n",
            "        16404, 16218, 16404, 15119, 11294,  3612,  3544,  2167,  6363,  5817,\n",
            "        12366, 15576, 14750,  6348, 12259, 12534, 16479,  1149,  4386, 12750,\n",
            "         7842,  1513, 16404, 15119,  3523,  1465,  5500,  1163, 10179,  1254,\n",
            "         2167,  4872,  8361, 13178, 16404,  4281, 13859, 12750,  5500, 16404,\n",
            "        10609, 12750,  8996, 11802,   443,  1005, 15318,  7047,  7274, 16404,\n",
            "         9400, 11420,  6625,  8996,  4796, 16626, 16285, 12534, 14698, 15121,\n",
            "        11165,  7047,  1662, 14970,  3809, 10858,  2167, 10424, 12750,  1662,\n",
            "        13551,  4225, 13182,  1737, 11035,  5166,  9343,  1737, 14995,  5166,\n",
            "        16404, 12534,  4909,  8266, 14378, 13702,  7047,  8996,  2089, 13551,\n",
            "         4225, 13182,  1737, 16612,  5166,  9343,  1737,   179,  2167,  5166,\n",
            "         4872, 10763,  8786, 16404,  8996,  5817, 13810, 11138,  5500,  7047,\n",
            "         8501,  4281,  1149, 12353, 12750, 10327, 10781, 12534,  8751, 16404,\n",
            "        12534,  4784,  1005,  1957, 10759,  7047,  6804, 11622,   505,   749,\n",
            "         2167, 11622,  6567, 12189,  8114, 16404,  1149,  5198, 12750,  8996,\n",
            "        16000, 29998])\n"
          ]
        }
      ],
      "source": [
        "raw_dataset = torch.load(\"raw_dataset.pt\")\n",
        "dataset = TextDataset(raw_dataset)\n",
        "# Print the first 5 items\n",
        "for i, (input, target) in enumerate(raw_dataset):\n",
        "    print(f\"Input: {input}\")\n",
        "    print(f\"Target: {target}\")\n",
        "    if i >= 4:  # stop after 5 items\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmLIMn6c2oN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsAm5TYhUT7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8cd8ba-34a8-46cf-dfce-b46ed4bdf507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model checkpoint:\n",
            "Key: encoder\n",
            "Value keys: odict_keys(['embedding.weight', 'self_attention.in_proj_weight', 'self_attention.in_proj_bias', 'self_attention.out_proj.weight', 'self_attention.out_proj.bias', 'feed_forward.0.weight', 'feed_forward.0.bias', 'feed_forward.2.weight', 'feed_forward.2.bias', 'norm1.weight', 'norm1.bias', 'norm2.weight', 'norm2.bias'])\n",
            "Key: decoder\n",
            "Value keys: odict_keys(['embedding.weight', 'self_attention.in_proj_weight', 'self_attention.in_proj_bias', 'self_attention.out_proj.weight', 'self_attention.out_proj.bias', 'feed_forward.0.weight', 'feed_forward.0.bias', 'feed_forward.2.weight', 'feed_forward.2.bias', 'norm1.weight', 'norm1.bias', 'norm2.weight', 'norm2.bias', 'norm3.weight', 'norm3.bias', 'out.weight', 'out.bias'])\n"
          ]
        }
      ],
      "source": [
        "# Load the model checkpoint\n",
        "checkpoint = torch.load(\"model.pt\")\n",
        "\n",
        "# Print all keys and values in the checkpoint\n",
        "print(\"Model checkpoint:\")\n",
        "for key, value in checkpoint.items():\n",
        "    print(f\"Key: {key}\")\n",
        "    if isinstance(value, dict):\n",
        "        # If the value is a dictionary (as it is for encoder and decoder state_dicts), print its keys\n",
        "        print(f\"Value keys: {value.keys()}\")\n",
        "    else:\n",
        "        # Otherwise, print the value itself\n",
        "        print(f\"Value: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference  Original Transformer"
      ],
      "metadata": {
        "id": "j_gDvmGQsAgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1    Training Data and BatchingWe trained on the standard WMT 2014 English-German dataset consisting of about **4.5 millions entence pairs.** Sentences were encoded using byte-pair encoding [3], which has a shared **source-target vocabulary of about 37000 tokens**. For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokens into a **32000 word-piece vocabulary** [38]. Sentence pairs were batched together by approximate sequence length. **Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.**"
      ],
      "metadata": {
        "id": "Mw8gjqClsFII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparaison\n",
        "\n",
        "Your Dataset:\n",
        "\n",
        "You have 15,000 words, and you are constructing sequences of 512 tokens for both input and target.\n",
        "This results in 14,488 sequence pairs, where each sequence pair consists of an input sequence of 512 tokens and a target sequence of 512 tokens.\n",
        "Therefore, the total number of tokens in a single sequence pair is 1,024 (512 + 512).\n",
        "Multiplying the total number of sequence pairs (14,488) by the number of tokens per sequence pair (1,024) gives you the total number of tokens in your entire dataset: 14,830,592.\n",
        "\n",
        "The original text describes two different training scenarios, one for English-German and another for English-French translation. Here's an analysis and comparison between the described scenarios and your current setup:\n",
        "\n",
        "### English-German Training:\n",
        "- **Dataset Size:** About 4.5 million sentence pairs.\n",
        "- **Vocabulary Size:** About 37,000 tokens, encoded using byte-pair encoding.\n",
        "- **Model Configuration:** Not mentioned explicitly, but the shared vocabulary size gives us some insight into the model's capabilities.\n",
        "\n",
        "### English-French Training:\n",
        "- **Dataset Size:** 36 million sentences.\n",
        "- **Vocabulary Size:** Split into a 32,000 word-piece vocabulary.\n",
        "- **Model Configuration:** Not explicitly stated.\n",
        "- **Batching:** Sentences batched by approximate length, with each batch containing approximately 25,000 source tokens and 25,000 target tokens (50,000 tokens total per batch).\n",
        "\n",
        "### Your Scenario:\n",
        "- **Dataset Size:** You've scraped around 15,748 words, with input sequences of 512 tokens, resulting in a dataset size of around 14,488 sequence pairs (assuming no overlap).\n",
        "- **Vocabulary Size:** Depending on the tokenization method used, the vocabulary size may vary.\n",
        "- **Model Configuration:** `d_model` is set to 512.\n",
        "- **Batching:** If you decide to match the batch size described for English-French (50,000 tokens per batch), you would have approximately 296 batches in your dataset.\n",
        "\n",
        "### Comparison:\n",
        "Your dataset is much smaller than either of the datasets described in the original text. The volume of data in terms of tokens and sentence pairs is significantly lower in your case.\n",
        "\n",
        "The described English-French training scenario is the one that mentions specific details about batching and token counts. Compared to that, your dataset's structure is different, and your total number of tokens (around 14.8 million) would be divided into different batch sizes, depending on how you decide to structure the training.\n",
        "\n",
        "In summary, while your setup shares some similarities in terms of handling tokens and sequence lengths, the scale and complexity of the datasets and possibly the model configurations are quite different. If your aim is to replicate or approximate the described training scenarios, you would likely need to consider adjustments in terms of the dataset size, vocabulary, and model architecture."
      ],
      "metadata": {
        "id": "e72czPWDxmTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2    Hardware and ScheduleWe trained our models on **one machine with 8 NVIDIA P100 GPUs.**  For our base models usingthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. Wetrained the base models for a total of**100,000 steps** or 12 hours. For our big models,(described on thebottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps(3.5 days)."
      ],
      "metadata": {
        "id": "_UiyQXb3sK1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.3    OptimizerWe used the **Adam optimizer** [20] withÎ²1= 0.9,Î²2= 0.98and\u000f= 10âˆ’9. We varied the learningrate over the course of training, according to the formula:lrate=dâˆ’0.5modelÂ·min(step_numâˆ’0.5,step_numÂ·warmup_stepsâˆ’1.5)(3)This corresponds to increasing the learning rate linearly for the firstwarmup_stepstraining steps,and decreasing it thereafter proportionally to the inverse square root of the step number. We usedwarmup_steps= 4000."
      ],
      "metadata": {
        "id": "mVgbOqDfsh7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.4    RegularizationWe employ **three types of regularization** during training:Residual **Dropout** We apply dropout [33] to the output of each sub-layer, before it is added to thesub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks. For the base model, we use a rate ofPdrop= 0.1."
      ],
      "metadata": {
        "id": "XUBAwo2Css1D"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyONJNqgk7DNZNL6LGVD725L"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
